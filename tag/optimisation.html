<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Pelican" name="generator"/>
<title>Duality Blog - optimisation</title>
<link href="https://manojbaishya.github.io/site.backup/theme/css/main.css" rel="stylesheet"/>
</head>
<body class="home" id="index">
<header class="body" id="banner">
<h1><a href="https://manojbaishya.github.io/site.backup/">Duality Blog</a></h1>
<nav><ul>
<li><a href="https://manojbaishya.github.io/site.backup/category/linear-algebra.html">linear-algebra</a></li>
</ul></nav>
</header><!-- /#banner -->
<aside class="body" id="featured">
<article>
<h1 class="entry-title"><a href="https://manojbaishya.github.io/site.backup/conjugate-gradient.html">Conjugate Gradient Methods</a></h1>
<footer class="post-info">
<abbr class="published" title="2021-01-23T12:30:00+05:30">
                Published: Sat 23 January 2021
        </abbr>
<address class="vcard author">
                By                         <a class="url fn" href="https://manojbaishya.github.io/site.backup/author/rahul-john-roy-piyush-mohapatra-manoj-baishya.html">Rahul John Roy, Piyush Mohapatra, Manoj Baishya</a>
</address>
<p>In <a href="https://manojbaishya.github.io/site.backup/category/linear-algebra.html">linear-algebra</a>.</p>
<p>tags: <a href="https://manojbaishya.github.io/site.backup/tag/linear-algebra.html">linear-algebra</a> <a href="https://manojbaishya.github.io/site.backup/tag/optimisation.html">optimisation</a> </p>
</footer><!-- /.post-info --><div class="section" id="quadratic-forms-and-linear-systems">
<h2>Quadratic Forms and Linear Systems</h2>
<p>Many important quantities arising in engineering are mathematically modelled as Quadratic Forms. Output noise power in Signal Processing, Kinetic and Potential Energy functions in Mechanical Engineering, Covariance Models of Random Variable Linear Dependence and Principal Component Analysis in Statistics, etc. are some of the frequently occurring applications of Quadratic Forms, illustrating their ubiquity and importance.</p>
<p>A Quadratic Form <span class="math">\(f(x)\)</span> on <span class="math">\(\mathbb{R}^n\)</span> is a function <span class="math">\(f : \mathbb{R}^n \rightarrow \mathbb{R}\)</span> such that:</p>
<div class="math">
\begin{align*}
f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T H \mathbf{x}  + \mathbf{c}^T x \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{c}, \mathbf{x} \in \mathbb{R}^n, H \in \mathbb{R}^{ n \times n} \text{ for simplicity} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
H = H^T, \text{ and } \mathbf{x}^T H \mathbf{x} &gt; 0 \: \forall \: \mathbf{x} \neq 0, \mathbf{x} \in \mathbb{R}^{\, n} \\
\end{align*}
</div>
<p>The graph of a positive-definite convex quadratic form is a hyperparaboloid in <span class="math">\(\mathbb{R}^{n + 1}\)</span> dimensions.</p>
<object data="/images/qform.svg" type="image/svg+xml">
</object>
<object data="/images/qform_contour.svg" type="image/svg+xml">
</object>
<p>Frequently, in many applications, we are interested in minimizing the quadratic form <span class="math">\(f(x)\)</span>, for example, minimising the energy function of a mechanical system so that it is in a stable state.</p>
<p>Suppose that, we have at our hands, a quadratic form obtained by mathematically modelling some quantity of interest and we wish to minimize it. At the minimum point  <span class="math">\(\mathbf{x}^*\)</span> (henceforth referred to as the minimizer), the gradient <span class="math">\(\nabla : \mathbb{R}^n \rightarrow \mathbb{R}\)</span> of a function <span class="math">\(f(x)\)</span> is always zero by first order optimality condition. Hence,</p>
<div class="math">
\begin{align*}
\nabla f(\mathbf{x}) = \frac{1}{2} \left[ H + H^T \right] \mathbf{x} + \mathbf{c} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\nabla f(\mathbf{x}) = H \mathbf{x} + \mathbf{c} \text{ since } H = H^T \\
\end{align*}
</div>
<p>At <span class="math">\(\mathbf{x}^*\)</span>, <span class="math">\(\nabla f(\mathbf{x}^*) = 0 \implies H \mathbf{x}^* + \mathbf{c} = 0 \text{ or } H \mathbf{x}^* = -\mathbf{c}\)</span>.</p>
<p>Thus, finding the minimum of the quadratic form <span class="math">\(f(\mathbf{x})\)</span> is equivalent to solving the linear system <span class="math">\(H \mathbf{x}^* = -\mathbf{c}\)</span> for the unknown <span class="math">\(\mathbf{x}^*\)</span>.</p>
<p>Again, suppose that we have at our hands a one-dimensional two-point boundary value problem (BVP), for example, the governing equations of flow of heat in a thin conducting rod with a source (<span class="math">\(t\)</span> is the continuous 1D spatial domain, <span class="math">\(x\)</span> is the temperature, <span class="math">\(g(t)\)</span> is the heat source):</p>
<div class="math">
\begin{align*}
\frac{d^2 x}{dt^2} = g(t) \\
\end{align*}
</div>
<h1 align="center">
<img src="/images/heat.png" width="500"/>
</h1><p>If we discretise the domain and convert the above differential equation to a finite difference equation, we obtain a linear system <span class="math">\(H \mathbf{x} = - \mathbf{c}\)</span>, where the temperatures <span class="math">\(\mathbf{x}\)</span> can be a very high dimensional vector due to the underlying problem being continuous. The <span class="math">\(H\)</span> represents the second-order derivative operator <span class="math">\(\frac{d^2 x}{dt^2}\)</span> and <span class="math">\(- \mathbf{c}\)</span> the heat source in the discretised domain <span class="math">\(\mathbf{R}^n\)</span>.</p>
<p>Since the BVP is continuous, the discretisation <span class="math">\(x\)</span> is made high-dimensional to capture its fidelity; therefore the matrix <span class="math">\(H \in \mathbb{R}^{n \times n}\)</span> is <strong>very large</strong>. More importantly, due to the structure and regularity of the 2<sup>nd</sup> order finite difference operator, we can make the following qualifiers: the matrix <span class="math">\(H\)</span> is <strong>sparse</strong>, <strong>symmetric</strong> and <strong>positive definite</strong>. Solving such a large linear system with a direct matrix factorisation method is prohibitively expensive (of the order of <span class="math">\(\mathcal{O}(\frac{2}{3} n^3)\)</span>). Hence, a better alternative is to solve the system iteratively and reach a solution in <span class="math">\(r &lt;&lt; n\)</span> steps that is close enough to the actual solution, which will cost us much less than a direct method.</p>
<p>One idea to solve the linear system <span class="math">\(H \mathbf{x} = - \mathbf{c}\)</span> is to use a stationary method, such as Jacobi or Gauss-Seidel. However, there is no guarantee of convergence, even for positive definite matrices.</p>
<p>A better approach is to utilize the observation made earlier: convert the linear system to its dual quadratic form and apply an iterative optimisation process on the objective function!</p>
<div class="math">
\begin{align*}
H \mathbf{x} = - \mathbf{c} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\Leftrightarrow \underset{\mathbf{x} \in \mathbb{R}^n}{\mathrm{min}} \: f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T H \mathbf{x}  + \mathbf{c}^T x \\
\end{align*}
</div>
<p>At the minimum of <span class="math">\(f(\mathbf{x})\)</span>, we would have found the solution to our original linear system.</p>
<p>We can summarise this section by making the following statement [See PCG Appendix C1 for proof]:</p>
<h1 align="center">
<img src="/images/eqv1.svg" width="700"/>
</h1></div>
<div class="section" id="line-search-techniques">
<h2>Line Search Techniques</h2>
<p>Now that we have our objective function, namely the quadratic form <span class="math">\(f : \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, our goal is to minimise it. We formalise our unconstrained optimisation problem <span class="math">\(\text{(P)}\)</span> as:</p>
<div class="math">
\begin{align*}
\text{(P)} \quad \underset{\mathbf{x} \in \mathbb{R}^n}{\mathrm{min}} \: f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T H \mathbf{x}  + \mathbf{c}^T x \\
\end{align*}
</div>
<dl class="docutils">
<dt>Before we begin, let us introduce some definitions:</dt>
<dd><ol class="first last arabic simple">
<li><strong>Error at the i-th iterate</strong>: <span class="math">\(\mathbf{e}_i = \mathbf{x}_i - \mathbf{x}^*\)</span></li>
<li><strong>Residual at the i-th iterate</strong>: <span class="math">\(\mathbf{r}_i = H \mathbf{x}_i + \mathbf{c}\)</span>, from linear system perspective.</li>
<li><strong>Gradient at the i-th iterate</strong>: <span class="math">\(\nabla f(\mathbf{x}_i) = H \mathbf{x}_i + \mathbf{c}\)</span>, from objective function perspective.</li>
</ol>
</dd>
<dt>We notice that:</dt>
<dd><ol class="first last arabic simple">
<li><span class="math">\(\mathbf{r}_i = H \mathbf{x}_i + \mathbf{c} = H \mathbf{x}_i - H \mathbf{x}^* = H (\mathbf{x}_i - \mathbf{x}^*) = H \mathbf{e}_i\)</span>, that is, the residual is simply the error mapped from the domain of <span class="math">\(H\)</span> to the range of <span class="math">\(H\)</span>.</li>
<li>The Residual of the Linear System is equal to the Gradient of the Objective Function. Throughout the article, whenever we mention residual, we also mention gradient to reinforce this fact.</li>
<li>Since <span class="math">\(\mathbf{x}^*\)</span> is unknown, <span class="math">\(\mathbf{e}_i\)</span> is unknown at every step, but the residuals <span class="math">\(\mathbf{r}_i\)</span> are <em>always known</em>. So whenever we want to use the error, we can simply work with the residual in the rangespace of <span class="math">\(H\)</span>.</li>
</ol>
</dd>
</dl>
<p>Now comes the core philosophy of Line Search: given our objective function <span class="math">\(f(\mathbf{x})\)</span>, we start with an initial guess <span class="math">\(\mathbf{x}_0\)</span>, and iterate our way downhill on <span class="math">\(f(\mathbf{x})\)</span> to reach <span class="math">\(\mathbf{x}^*\)</span>. At any i-th iterate, we are at the point <span class="math">\(\mathbf{x}_i\)</span>, and to travel to our next point <span class="math">\(\mathbf{x}_{i + 1}\)</span>, we must choose a direction of descent <span class="math">\(\mathbf{p}_i\)</span>, and then move a step length <span class="math">\(\alpha_i\)</span> the right amount so that along this direction, <span class="math">\(f(\mathbf{x}_{i + 1}) = \phi(\alpha_i)\)</span> is minimum. Mathematically,</p>
<div class="math">
\begin{align*}
\mathbf{x}_{i + 1} = \mathbf{x}_i + \alpha_i \mathbf{p}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\alpha_i = \underset{x_{i + 1}}{\mathrm{argmin}} \: f(\mathbf{x}_{i + 1}) = \underset{\alpha_{i}}{\mathrm{argmin}} \: f(\mathbf{x}_i + \alpha_i \mathbf{p}_i) = \underset{\alpha_{i}}{\mathrm{argmin}} \: \phi(\alpha_i) \\
\end{align*}
</div>
<p>One approach is to use the Steepest Descent (SD) technique to compute and move along a direction <span class="math">\(\mathbf{p}_i\)</span> towards vanishing gradient:</p>
<div class="math">
\begin{align*}
\mathbf{p}_i = - \nabla f(\mathbf{x}_i) = -  \mathbf{r}_i \\
\end{align*}
</div>
<p>SD moves along (or opposite to be precise) the residual/gradient vector at each iterate. One fact we state without proof [see PCG pg. 6; can be derived from 1D calculus] is that the residual/gradient at the current iterate <strong>i</strong> is always orthogonal to the previous residual/gradient at <strong>i - 1</strong>.</p>
<h1 align="center">
<img src="/images/sd.png" width="600"/>
</h1><p>As can be seen from the above picture, this is unfortunate, because we repeat directions in multiple steps a lot, and the number of steps needed to converge to the minimum point <span class="math">\(\mathbf{x}^*\)</span> is extraordinarily large. This could have been avoided if, after taking a new direction each time, we had travelled just the right amount of length in that direction so that in our future march downhill, we never need to step in that direction again.</p>
<p>This raises an important question: If we picked a direction and took the RIGHT step length along it to avoid repeating it, in how many steps will be reach the minimizer <span class="math">\(\mathbf{x}^*\)</span>?</p>
<p>The answer is: n steps! (the dimension of the vectorspace). To see why it is so, we need to wait for some more explanations.</p>
<p>Let us the explore the above idea a bit further in the following sections.</p>
</div>
<div class="section" id="coordinate-descent-method">
<h2>Coordinate Descent Method</h2>
<p>To demonstrate the idea of n-directions, let us solve two problems and analyse their solutions. As a first attempt, we will pick the coordinate axes (i.e. the canonical basis) as the directions of descent.</p>
<div class="section" id="problem-1">
<h3>Problem 1</h3>
<div class="math">
\begin{align*}
\underset{x \in \mathbb{R}^2}{\mathrm{min}} \: f(\mathbf{x}) = 4 x_1^2 + x_2^2 = \frac{1}{2} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}^T \begin{bmatrix} 8 &amp; 0\\ 0 &amp; 2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix}^T \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \\
\end{align*}
</div>
<p>The minimum point is at <span class="math">\(\mathbf{x}^* = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span>.</p>
<p>Let us select our initial guess as <span class="math">\(\mathbf{x}_0 = \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span>. Our first direction of motion is <span class="math">\(\mathbf{p}_0 = \mathbf{e}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span>. Therefore, <span class="math">\(\mathbf{x}_1 =  \mathbf{x}_0  +  \alpha_0 \mathbf{p}_0 = \begin{bmatrix} -1 \\ -1 \end{bmatrix} +  \alpha_0 \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} -1 + \alpha_0 \\ -1 \end{bmatrix}\)</span> and <span class="math">\(\phi(\alpha_0) = f(\mathbf{x}_1) = 4 (-1 + \alpha_0)^2 + (-1)^2\)</span>. Taking the derivative of <span class="math">\(\phi(\alpha_0)\)</span> with respect to <span class="math">\(\alpha_0\)</span>, setting it to zero and solving the equation, we obtain <span class="math">\(\alpha_0 = 1\)</span>. Therefore, <span class="math">\(\mathbf{x}_1 = \begin{bmatrix} 0 \\ -1 \end{bmatrix}\)</span>. Repeating this process for the next direction <span class="math">\(\mathbf{p}_1 = \mathbf{e}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}\)</span>, we obtain <span class="math">\(\mathbf{x}_2 = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span>, which is indeed our minimum <span class="math">\(\mathbf{x}^*\)</span>. We reached our destination in two steps! (= dimension of our decision space <span class="math">\(\mathbb{R}^2\)</span>)</p>
<p>The contours of the function and our iteration path are as displayed below:</p>
<h1 align="center">
<img src="/images/p1_solved.svg" width="700"/>
</h1></div>
<div class="section" id="problem-2">
<h3>Problem 2</h3>
<div class="math">
\begin{align*}
\underset{x \in \mathbb{R}^2}{\mathrm{min}} \: f(\mathbf{x}) = 4 x_1^2 + x_2^2 - 2 x_1 x_2 = \frac{1}{2} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}^T \begin{bmatrix} 8 &amp; -2 \\ -2 &amp; 2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix}^T \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \\
\end{align*}
</div>
<p>The minimum point is again at <span class="math">\(\mathbf{x}^* = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span>.</p>
<p>We repeat the same process as outlined in Problem 1, starting from  <span class="math">\(\mathbf{x}_0 = \begin{bmatrix} -1 \\ -1 \end{bmatrix}\)</span>, and picking <span class="math">\(\mathbf{e}_1\)</span> and then <span class="math">\(\mathbf{e}_2\)</span> as search directions. However, this time, we obtain the iterates as <span class="math">\(\mathbf{x}_1 = \begin{bmatrix} \frac{-1}{4} \\ -1 \end{bmatrix}\)</span>, and <span class="math">\(\mathbf{x}_2 = \begin{bmatrix} \frac{-1}{4} \\ \frac{-1}{4} \end{bmatrix}\)</span>. Notice that, we haven't reached our minimizer in two steps.</p>
<h1 align="center">
<img src="/images/p2_solved.svg" width="700"/>
</h1><p>Let us spend sometime looking over what we have done so far and why our coordinate axes approach failed in the second problem. Notice that, in Problem 1, the Hessian Matrix (ie the second derivative of a multivariate scalar function) of the Quadratic Form is diagonal, and hence our coordinate axes are aligned with its eigenvectors. Not only that, we know that for a symmetric matrix, the eigenvectors are always orthogonal, that is, <span class="math">\(\mathbf{v}_i^T \mathbf{v}_j = \left\{     \begin{array}{ll} \text{ non-zero }  &amp; \mbox{if } i = j \\  0 &amp; \mbox{if } i \neq j   \end{array} \right. \: \forall \: i, j \: \in \: [1 \dots n]\)</span>.</p>
<p>We can write the minimizer as <span class="math">\(\mathbf{x}^* = \mathbf{x}_0 + \alpha_0 \mathbf{p}_0 + \alpha_1 \mathbf{p}_1 = \mathbf{x}_0 + \alpha_0 \mathbf{v}_1 + \alpha_1 \mathbf{v}_2\)</span>. Substitute in <span class="math">\(\mathbf{x}^*\)</span> in the objective function and we will get:</p>
<div class="math">
\begin{align*}
\underset{x \in \mathbb{R}^2}{\mathrm{min}} \: f(\mathbf{x}^*) = \frac{1}{2} (\mathbf{x}_0 + \alpha_0 \mathbf{v}_1 + \alpha_1 \mathbf{v}_2)^T \begin{bmatrix} 8 &amp; 0\\ 0 &amp; 2 \end{bmatrix} (\mathbf{x}_0 + \alpha_0 \mathbf{v}_1 + \alpha_1 \mathbf{v}_2) \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \underset{x \in \mathbb{R}^2}{\mathrm{min}} \: f(\mathbf{x}^*) = \frac{1}{2} (\mathbf{x}_0 + \alpha_0 \mathbf{v}_1 + \alpha_1 \mathbf{v}_2)^T H (\mathbf{x}_0 + \alpha_0 \mathbf{v}_1 + \alpha_1 \mathbf{v}_2) \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \underset{x \in \mathbb{R}^2}{\mathrm{min}} \: f(\mathbf{x}^*) = \frac{1}{2} (\mathbf{x}_0 + \alpha_0 \mathbf{v}_1 + \alpha_1 \mathbf{v}_2)^T (H\mathbf{x}_0 + \alpha_0 H \mathbf{v}_1 + \alpha_1 H \mathbf{v}_2) \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \underset{x \in \mathbb{R}^2}{\mathrm{min}} \: f(\mathbf{x}^*) = \frac{1}{2} \mathbf{x}_0^T (H\mathbf{x}_0 + \alpha_0 \lambda_1 \mathbf{v}_1 + \alpha_1 \lambda_2 \mathbf{v}_2) + \frac{1}{2} (\alpha_0 \mathbf{v}_1 + \alpha_1 \mathbf{v}_2)^T H\mathbf{x}_0 + \\
\frac{1}{2} (\alpha_0 \mathbf{v}_1 + \alpha_1 \mathbf{v}_2)^T (\alpha_0 H \mathbf{v}_1 + \alpha_1 H\mathbf{v}_2) \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \underset{x \in \mathbb{R}^2}{\mathrm{min}} \: f(\mathbf{x}^*) = \frac{1}{2} \mathbf{x}_0^T H\mathbf{x}_0 +  \frac{1}{2} \alpha_0 \lambda_1 \mathbf{x}_0^T \mathbf{v}_1 + \frac{1}{2} \alpha_1 \lambda_2 \mathbf{x}_0^T \mathbf{v}_2 + \frac{1}{2} \alpha_0 \mathbf{v}_1^T H \mathbf{x}_0 + \frac{1}{2} \alpha_1 \mathbf{v}_2^T H \mathbf{x}_0 + \\
\left[ \frac{1}{2} \alpha_0^2 \mathbf{v}_1^T H \mathbf{v}_1 + \frac{1}{2} \alpha_0 \alpha_1 \mathbf{v}_1^T H \mathbf{v}_2 + \frac{1}{2} \alpha_1 \alpha_0 \mathbf{v}_2^T H \mathbf{v}_1 + \frac{1}{2} \alpha_1^2 \mathbf{v}_2^T H \mathbf{v}_2 \right] \\
\end{align*}
</div>
<p>Let us pause for a moment here. The above equation looks very complicated, by we have to remember that <span class="math">\(\mathbf{v}_1\)</span> and <span class="math">\(\mathbf{v}_2\)</span> are eigenvectors of <span class="math">\(H\)</span>. Hence, the terms containing <span class="math">\(\mathbf{v}_1^T H \mathbf{v}_2\)</span> and <span class="math">\(\mathbf{v}_2^T H \mathbf{v}_1\)</span> become, <span class="math">\(\mathbf{v}_1^T H \mathbf{v}_2 = \mathbf{v}_1^T \lambda_2 \mathbf{v}_2 = \lambda_2 \mathbf{v}_1^T \mathbf{v}_2 = 0\)</span> and <span class="math">\(\mathbf{v}_2^T H \mathbf{v}_1 = \mathbf{v}_2^T \lambda_1 \mathbf{v}_1 = \lambda_1 \mathbf{v}_2^T \mathbf{v}_1 = 0\)</span>, due to the orthogonality of the eigenvectors. Hence, after some simplifications, we get,</p>
<div class="math">
\begin{align*}
\implies \underset{x \in \mathbb{R}^2}{\mathrm{min}} \: f(\mathbf{x}^*) = \frac{1}{2} \mathbf{x}_0^T H\mathbf{x}_0 +  \alpha_0 \lambda_1 \mathbf{x}_0^T \mathbf{v}_1 + \alpha_1 \lambda_2 \mathbf{x}_0^T \mathbf{v}_2 + \frac{1}{2} (\alpha_0^2 \lambda_1 \mathbf{v}_1^T \mathbf{v}_1 + \alpha_1^2 \lambda_2 \mathbf{v}_2^T \mathbf{v}_2) \\
\end{align*}
</div>
<p>In the derivation above, the products <span class="math">\(\mathbf{v}_i^T H \mathbf{v}_j\)</span> became 0 whenever <span class="math">\(i \neq j\)</span>, but survived otherwise if <span class="math">\(i = j\)</span>. There are no mixed terms containing both directions <span class="math">\(\mathbf{v}_1\)</span> and <span class="math">\(\mathbf{v}_2\)</span> together, only in isolation, and we know that we have reached the minimiser using this technique.</p>
<p>We will do the same exercise for Problem 2. However, since the coordinate axes are not aligned with the eigenvectors of <span class="math">\(H\)</span> because it is not diagonal, we cannot do the <span class="math">\(\mathbf{p}_i = \mathbf{v}_{i + 1}\)</span> substitutions here. Therefore, (skipping some steps),</p>
<div class="math">
\begin{align*}
\underset{x \in \mathbb{R}^2}{\mathrm{min}} \: f(\mathbf{x}) = \frac{1}{2} (\mathbf{x}_0 + \alpha_0 \mathbf{p}_0 + \alpha_1 \mathbf{p}_1)^T \begin{bmatrix} 8 &amp; -2\\ -2 &amp; 2 \end{bmatrix} (\mathbf{x}_0 + \alpha_0 \mathbf{p}_0 + \alpha_1 \mathbf{p}_1) \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \underset{x \in \mathbb{R}^2}{\mathrm{min}} \: f(\mathbf{x}) = \frac{1}{2} \mathbf{x}_0^T H\mathbf{x}_0 +  \frac{1}{2} \alpha_0 \mathbf{x}_0^T H \mathbf{p}_0 + \frac{1}{2} \alpha_1 \mathbf{x}_0^T H \mathbf{p}_1 + \frac{1}{2} \alpha_0 \mathbf{p}_0^T H \mathbf{x}_0 + \frac{1}{2} \alpha_1 \mathbf{p}_1^T H \mathbf{x}_0 + \\
\left[ \frac{1}{2} \alpha_0^2 \mathbf{p}_0^T H \mathbf{p}_0 + \frac{1}{2} \alpha_0 \alpha_1 \mathbf{p}_0^T H \mathbf{p}_1 + \frac{1}{2} \alpha_1 \alpha_0 \mathbf{p}_1^T H \mathbf{p}_0 + \frac{1}{2} \alpha_1^2 \mathbf{p}_1^T H \mathbf{p}_1 \right] \\
\end{align*}
</div>
<p>Again, let us compare this expression with that of Problem 1. The terms outside brackets are exactly the same for both problems. So, should not we reach the minimiser in this case also? If we look inside the sum inside the brackets, it is observed that the cross terms <span class="math">\(\mathbf{p}_i^T H \mathbf{p}_j \: \forall \: i \neq j\)</span> are still present. This is <em>the only difference</em> between the two problems, and we have witnessed that one led to the solution, the other did not.</p>
<p>This leads us to a good start. We can take the eigenvectors of the Hessian <span class="math">\(H\)</span> (SPD matrices always have a full set of n eigenvectors) and move along those directions, and we are guaranteed to reach the minimiser in n steps. BUT! Calculating eigenvectors of a matrix is very expensive. Using direct matrix factorisation sounds much better than this approach!</p>
<p>We also cannot use the residuals as search directions, as that will take us back to the Steepest Descent, and moreover they will not be linearly independent due to repetitions.</p>
<p>So what we will do instead is, we will build a set of <strong>n</strong> linear independent set of directions <span class="math">\(\mathbf{p}_i\)</span>, and impose the desirable property observed earlier from the eigenvectors that <span class="math">\(\mathbf{p}_i^T H \mathbf{p}_j = \left\{     \begin{array}{ll} \text{ non-zero }  &amp; \mbox{if } i = j \\  0 &amp; \mbox{if } i \neq j   \end{array} \right. \: \forall \: i, j \: \in \: [1 \dots n]\)</span>. What we hope to achieve is the same effect we observed in Problem 1: no mixed direction terms in the objective function at the minimiser, with guaranteed convergence in n-steps.</p>
<p>This is the precisely the definition of Conjugate Directions! The idea is very simple, instead of clueless wandering in search of our destination by only using local gradient information, we intelligently exploit the nature of our problem to fix all our search directions in one shot. After <strong>n</strong> searches, we are done and can go home.</p>
<p>Before we move ahead, let us spend a little more time on Problem 1. Observe that, when moving from iterate <span class="math">\(\mathbf{x}_0\)</span> to <span class="math">\(\mathbf{x}_1\)</span>, the error <span class="math">\(\mathbf{e}_1 = \mathbf{x}_1 - \mathbf{x}^*\)</span> is orthogonal to the the direction <span class="math">\(\mathbf{p}_0\)</span>. This is another desirable property we will use in n-direction approach. However, we don't know <span class="math">\(\mathbf{x}^*\)</span>, so we don't know <span class="math">\(\mathbf{e}_{i + 1}\)</span>, and as noted in the previous section, we will instead make the residual/gradient <span class="math">\(\mathbf{r}_{i + 1} = H \mathbf{e}_{i + 1}\)</span> orthogonal to the the previous search directions <span class="math">\(\mathbf{p}_i\)</span>.</p>
</div>
</div>
<div class="section" id="descent-along-conjugate-directions-a-powerful-line-search-method">
<h2>Descent along Conjugate Directions: A Powerful Line Search Method</h2>
<p>Two vectors, <span class="math">\(\mathbf{p}_i\)</span> and <span class="math">\(\mathbf{p}_j\)</span> are said to be <em>H-conjugate</em> with respect to some symmetric, positive definite matrix <span class="math">\(H\)</span>, if they satisfy,</p>
<blockquote>
<div class="math">
\begin{align*}
\mathbf{p}_i^T H \mathbf{p}_j = \left\{     \begin{array}{ll} \text{ non-zero }  &amp; \mbox{if } i = j \\  0 &amp; \mbox{if } i \neq j   \end{array} \right. \: \forall \: i, j \: \in \: [1 \dots n] \\
\end{align*}
</div>
</blockquote>
<p>From the discussion in the previous section, we outline our goals and issues for the rest of the article as follows:</p>
<ol class="arabic simple">
<li>Find a set of <strong>n</strong> linearly independent set of <em>H-conjugate</em> directions <span class="math">\(\mathbf{p}_i\)</span>.</li>
<li>Establish relationships between residuals/gradients, search directions and the vectorspace of our solution <span class="math">\(\mathbf{x}^*\)</span>.</li>
<li>Use the above two rules to calculate the step lengths <span class="math">\(\alpha_i\)</span> along <span class="math">\(\mathbf{p}_i\)</span>.</li>
</ol>
<p>To repeat, we are solving the unconstrained optimisation problem:</p>
<div class="math">
\begin{align*}
\text{(P)} \quad \underset{\mathbf{x} \in \mathbb{R}^n}{\mathrm{min}} \: f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T H \mathbf{x}  + \mathbf{c}^T x \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{c}, \mathbf{x} \in \mathbb{R}^n, H \in \mathbb{R}^{ n \times n} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
H = H^T, \text{ and } \mathbf{x}^T H \mathbf{x} &gt; 0 \: \forall \: \mathbf{x} \neq 0, \mathbf{x} \in \mathbb{R}^{\, n} \\
\end{align*}
</div>
<p>The minimiser of our problem <span class="math">\(\text{(P)}\)</span> can be stated as:</p>
<div class="math">
\begin{align*}
\mathbf{x}^* = \mathbf{x}_0 + \sum_{i = 0}^{n - 1} \alpha_i \mathbf{p}_i \\
\end{align*}
</div>
<p>Our one-line iterative procedure is:</p>
<div class="math">
\begin{align*}
\mathbf{x}_{i + 1} = \mathbf{x}_i + \alpha_i \mathbf{p}_i \\
\end{align*}
</div>
<p>Now we will slowly build up our procedure, step by step.</p>
<p>Recall that we want to make residual/gradient at the next iterate <span class="math">\(i + 1\)</span> orthogonal to our direction at the current iterate <span class="math">\(i\)</span>.</p>
<div class="math">
\begin{align*}
\mathbf{p}_i^T \mathbf{r}_{i + 1} = 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{p}_i^T H \mathbf{e}_{i + 1} = 0 \\
\end{align*}
</div>
<p>Note that the error at the next iterate is H-conjugate to the previous direction. We will have more to say about this later.</p>
<p>Using our definitions, we have:</p>
<div class="math">
\begin{align*}
\mathbf{x}_{i + 1} = \mathbf{x}_i + \alpha_i \mathbf{p}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{x}_{i + 1} - \mathbf{x}^* = \mathbf{x}_i  - \mathbf{x}^* + \alpha_i \mathbf{p}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{e}_{i + 1} = \mathbf{e}_i + \alpha_i \mathbf{p}_i \\
\end{align*}
</div>
<p>Substituting in the previous equation, we get:</p>
<div class="math">
\begin{align*}
\implies \mathbf{p}_i^T H \mathbf{e}_{i + 1} = 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{p}_i^T H (\mathbf{e}_i + \alpha_i \mathbf{p}_i) = 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{p}_i^T H \mathbf{e}_i + \alpha_i \mathbf{p}_i^T H \mathbf{p}_i = 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{p}_i^T \mathbf{r}_i + \alpha_i \mathbf{p}_i^T H \mathbf{p}_i = 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \alpha_i = - \frac{\mathbf{p}_i^T \mathbf{r}_i}{\mathbf{p}_i^T H \mathbf{p}_i}, \text{ where } \mathbf{r}_i = \nabla f(\mathbf{x}_i) = H \mathbf{x}_i + \mathbf{c} \\
\end{align*}
</div>
<p>All of the terms in the right hand side of the above expression are known at the current iterate. This gives us our step length
<span class="math">\(\alpha_i\)</span>, our goal #3!</p>
<p>Okay, we admit that it was arbitrary. Why should us wanting to orthogonalise the next residual to the current search direction give us a credible answer for the step length? So, let us try something else: given that we are standing at <span class="math">\(\mathbf{x}_i\)</span>, we want to minimise our objective <span class="math">\(f(\mathbf{x})\)</span> at <span class="math">\(\mathbf{x}_{i + 1} = \mathbf{x}_i + \alpha_i \mathbf{p}_i\)</span> over the direction <span class="math">\(\mathbf{p}_i\)</span> (assume it is somehow already calculated). So, we can write:</p>
<div class="math">
\begin{align*}
\underset{\mathbf{x}_{i + 1} \in \mathbb{R}^n}{\mathrm{min}} \: f(\mathbf{x}_{i + 1}) =  \underset{\alpha_i \in \mathbb{R}}{\mathrm{min}} \: \phi(\alpha_i) = \frac{1}{2} (\mathbf{x}_i + \alpha_i \mathbf{p}_i)^T H (\mathbf{x}_i + \alpha_i \mathbf{p}_i)  + \mathbf{c}^T (\mathbf{x}_i + \alpha_i \mathbf{p}_i) \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \underset{\alpha_i \in \mathbb{R}}{\mathrm{min}} \: \phi(\alpha_i) = \frac{1}{2} \mathbf{x}_i^T H \mathbf{x}_i + \frac{1}{2} \alpha_i^2 \mathbf{p}_i^T H \mathbf{p}_i + \alpha_i \mathbf{p}_i^T H \mathbf{x}_i + \mathbf{c}^T \mathbf{x}_i + \alpha_i \mathbf{c}^T \mathbf{p}_i \\
\end{align*}
</div>
<p>Now comes our calculus attack: we will differentiate <span class="math">\(\phi(\alpha_i)\)</span> with respect to <span class="math">\(\alpha_i\)</span> and set the derivative to zero to find the directional minimum (Why? Because we know <span class="math">\(\mathbf{x}_i\)</span>, we assumed that we already know <span class="math">\(\mathbf{p}_i\)</span>, and we just need to find the minimum along this direction)</p>
<div class="math">
\begin{align*}
\nabla_{\alpha_i} \phi(\alpha_i) = \frac{\partial \phi}{\partial \alpha_i} = \alpha_i \mathbf{p}_i^T H \mathbf{p}_i + \mathbf{p}_i^T H \mathbf{x}_i + \mathbf{c}^T \mathbf{p}_i \rightarrow 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \alpha_i \mathbf{p}_i^T H \mathbf{p}_i + \mathbf{p}_i^T H \mathbf{x}_i +  \mathbf{p}_i^T \mathbf{c} = 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \alpha_i \mathbf{p}_i^T H \mathbf{p}_i + \mathbf{p}_i^T (H \mathbf{x}_i + \mathbf{c}) = 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \alpha_i = - \frac{\mathbf{p}_i^T (H \mathbf{x}_i + \mathbf{c})}{\mathbf{p}_i^T H \mathbf{p}_i} \\
\end{align*}
</div>
<p>which is nothing but (see definitions in Section II):</p>
<div class="math">
\begin{align*}
\implies \alpha_i = - \frac{\mathbf{p}_i^T \mathbf{r}_i}{\mathbf{p}_i^T H \mathbf{p}_i} \text{ !} \\
\end{align*}
</div>
<p>We have now arrived at the same step length as when we arbitrarily forced the next residual to be perpendicular to the current direction. Now, we know better: this step length actually minimises our objective along the current direction, and the former fact just comes about as a corollary! Further, we note that <span class="math">\(\mathbf{r}_{i + 1} \perp \mathbf{p}_i \implies \mathbf{p}_i^T \mathbf{r}_{i + 1} = \mathbf{p}_i^T H \mathbf{e}_{i + 1} = 0\)</span>, that is, direction - residual orthogonality implies direction - error H-conjugacy. In fact, we will make the following hand-waving argument: since the error at the next iterate is always H-conjugate to the current search direction, the current search direction is H-conjugate to all the previous search directions, and we don't ever step into a previously encountered direction again in the future, the next error is evermore H-conjugate to all the previous search directions by recursion. Which implies the residual is orthogonal to all the previous search directions! The rigorous proof of this statement is contained in the Expanding Subspace Minimization Theorem, which is presented in the next section.</p>
<p>Before we march ahead, let us establish a relationship between the residuals. We shall use this later in proving some theoretical guarantees. Using our error relationship, premultiply both sides of the equation by <span class="math">\(H\)</span>:</p>
<div class="math">
\begin{align*}
\mathbf{e}_{i + 1} = \mathbf{e}_i + \alpha_i \mathbf{p}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies H \mathbf{e}_{i + 1} = H \mathbf{e}_i + \alpha_i H \mathbf{p}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{r}_{i + 1} = \mathbf{r}_i + \alpha_i H \mathbf{p}_i \\
\end{align*}
</div>
<p>Next, we will try to find the directions <span class="math">\(\mathbf{p}_i\)</span>. Let us pick, from our available knowledge, n linearly independent vectors <span class="math">\(\{ \mathbf{u}_0, \mathbf{u}_1, \dots, \mathbf{u}_{n - 1} \}\)</span>, which we will call as the <strong>generator vectors</strong>. The easiest choice we can make is the coordinate axes, <span class="math">\(\{ \mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n \}\)</span>. To construct <span class="math">\(\mathbf{p}_i\)</span>, we will take each generator <span class="math">\(\mathbf{u}_i\)</span> and peel off any components that are not H-conjugate to the entire set of previous <span class="math">\(i - 1\)</span> <span class="math">\(\mathbf{p}\)</span> vectors. To kickstart the process, set <span class="math">\(\mathbf{p}_0 = \mathbf{u}_0 = \mathbf{e}_1\)</span> and for all <span class="math">\(i &gt; 1\)</span>, iterate as:</p>
<div class="math">
\begin{align*}
\mathbf{p}_i = \mathbf{u}_i + \sum_{k = 0}^{i - 1} \beta_{ik} \mathbf{p}_k, \: \beta_{ik} \text{ defined for } i &gt; k \\
\end{align*}
</div>
<p>To calculate the <span class="math">\(\beta_{ik}\)</span> factors, we exploit the H-conjugacy property of <span class="math">\(\mathbf{p}_i\)</span> directions. Transpose above equation and post-multiply with <span class="math">\(H \mathbf{p}_j, j \in [0, \dots, i - 1]\)</span>:</p>
<div class="math">
\begin{align*}
\mathbf{p}_i^T H \mathbf{p}_j = \mathbf{u}_i^T H \mathbf{p}_j + \sum_{k = 0}^{i - 1} \beta_{ik} \mathbf{p}_k^T H \mathbf{p}_j \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies 0 = \mathbf{u}_i^T H \mathbf{p}_j + \beta_{ij} \mathbf{p}_j^T H \mathbf{p}_j \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \beta_{ij} = - \frac{\mathbf{u}_i^T H \mathbf{p}_j}{\mathbf{p}_j^T H \mathbf{p}_j} \\
\end{align*}
</div>
<p>Unfortunately, this means that to calculate each <span class="math">\(\mathbf{p}_i\)</span> upto the n-th direction, all the previous <span class="math">\(H \mathbf{p}_j, j \in [0, \dots, i - 1]\)</span> directions must be kept in storage. This is not very efficient. In fact, the total cost of generating all the search directions is <span class="math">\(\mathcal{O}(n^3)\)</span>), making direct factorisation look appealing! This brings us to the topic of our next section: <strong>Conjugate Gradients</strong> (or <strong>Conjugate(d) Gradients</strong> / <strong>Conjugate(d) Residuals</strong>), a very efficient method of constructing Conjugate Directions.</p>
<p>But wait! Did we not mention before, this conjugate directions process will guarantee convergence in n-steps? Or will it, really? It sounds quite arbitrary. Are we full of ourselves? Why should it happen at all? Let's attack this notion with a mathematical analysis!</p>
<p><em>We will try to do this</em>: first we will assume that the process indeed converges in n-steps, which implies, the initial error term <span class="math">\(\mathbf{e}_0\)</span> is made up of n-vectors oriented along these directions <span class="math">\(\mathbf{p}_i\)</span>. Then, we shall use the H-conjugacy properties of the directions. If we find that, <span class="math">\(\mathbf{e}_n\)</span>, that is error after n-steps, is not zero, our big claim is grossly incorrect, and we will terribly fall from grace. However, if <span class="math">\(\mathbf{e}_0\)</span> is indeed zero, we would have proven our n-step convergence assumption on stone and are right on target!</p>
<p>Without much ado, let's formulate the math:</p>
<div class="math">
\begin{align*}
\mathbf{e}_0 = \sum_{j = 0}^{n - 1} \: \delta_j \mathbf{p}_j \\
\end{align*}
</div>
<p>Premultiplying <span class="math">\(\mathbf{p}_k^T H\)</span> and exploiting H-conjugacy, <span class="math">\(\mathbf{p}_k^T H \mathbf{p}_j = 0 \: \forall \: k \neq j\)</span> :</p>
<div class="math">
\begin{align*}
\mathbf{p}_k^T H \mathbf{e}_0 = \sum_{j = 0}^{n - 1} \: \delta_j \mathbf{p}_k^T H \mathbf{p}_j \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{p}_k^T H \mathbf{e}_0 =  \delta_k \mathbf{p}_k^T H \mathbf{p}_k \\
\end{align*}
</div>
<p>Using the fact that <span class="math">\(\sum_{i = 0}^{k - 1} \: \alpha_i \mathbf{p}_k^T H \mathbf{p}_i = 0\)</span>,</p>
<div class="math">
\begin{align*}
\implies \mathbf{p}_k^T H \mathbf{e}_0 + \sum_{i = 0}^{k - 1} \: \alpha_i \mathbf{p}_k^T H \mathbf{p}_i =  \delta_k \mathbf{p}_k^T H \mathbf{p}_k \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{p}_k^T H (\mathbf{e}_0 + \sum_{i = 0}^{k - 1} \: \alpha_i \mathbf{p}_i) =  \delta_k \mathbf{p}_k^T H \mathbf{p}_k \\
\end{align*}
</div>
<p>We also know that, <span class="math">\(\mathbf{e}_{i + 1} = \mathbf{e}_i + \alpha_i \mathbf{p}_i\)</span> or <span class="math">\(\mathbf{e}_k = \mathbf{e}_{k - 1} + \alpha_{k - 1} \mathbf{p}_{k - 1}\)</span>. Recursively expanding this formula into a summation, we get, <span class="math">\(\mathbf{e}_k = \mathbf{e}_{k - 1} + \alpha_{k - 1} \mathbf{p}_{k - 1} = \mathbf{e}_0 + \sum_{i = 0}^{k - 1} \: \alpha_i \mathbf{p}_i\)</span>. Substituting this into the above expression:</p>
<div class="math">
\begin{align*}
\implies \mathbf{p}_k^T H \mathbf{e}_k =  \delta_k \mathbf{p}_k^T H \mathbf{p}_k \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \delta_k = \frac{\mathbf{p}_k^T \mathbf{r}_k}{\mathbf{p}_k^T H \mathbf{p}_k} \\
\end{align*}
</div>
<p>This means:</p>
<div class="math">
\begin{align*}
\delta_k = - \alpha_k \text{ ! } \\
\end{align*}
</div>
<p>Moving ahead,</p>
<div class="math">
\begin{align*}
\mathbf{e}_k = \mathbf{e}_0 + \sum_{i = 0}^{k - 1} \: \alpha_i \mathbf{p}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{e}_k = \sum_{j = 0}^{n - 1} \: \delta_j \mathbf{p}_j - \sum_{i = 0}^{k - 1} \: \delta_i \mathbf{p}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{e}_k = \sum_{j = 0}^{k - 1} \: \delta_j \mathbf{p}_j + \sum_{l = k}^{n - 1} \: \delta_l \mathbf{p}_l - \sum_{i = 0}^{k - 1} \: \delta_i \mathbf{p}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{e}_k = \sum_{l = k}^{n - 1} \: \delta_l \mathbf{p}_l \\
\end{align*}
</div>
<p>Substituting <span class="math">\(k = n\)</span>,</p>
<div class="math">
\begin{align*}
\mathbf{e}_n = \sum_{l = n}^{n - 1} \: \delta_l \mathbf{p}_l = 0 \\
\end{align*}
</div>
<p>because we do not have any directions beyond the (n - 1)-th iterate.</p>
<p>Therefore, the error at the n-th step is really zero, and we have arrived at this conclusion by just using the H-conjugacy property! Thus, we can safely state that this Conjugate Directions method indeed converges in n-steps, as we have been advertising. (Disclaimer: The problem has to be convex quadratic positive-definite!)</p>
<p>Okay, now we can move forward to the next sections in confidence and peace of mind.</p>
</div>
<div class="section" id="some-proofs-on-our-road-to-enlightenment">
<h2>Some Proofs on our Road to Enlightenment</h2>
<div class="section" id="expanding-subspace-minimization-esm-theorem">
<h3>Expanding Subspace Minimization (ESM) Theorem</h3>
<p>Let <span class="math">\(\mathbf{x}_0 \in \mathbb{R}^n\)</span>. Given a set of H-conjugate <span class="math">\(\{ \mathbf{p}_k \}_0^{i - 1}\)</span> directions, and <span class="math">\(\mathbf{x}_k = \mathbf{x}_{k} = \mathbf{x}_{k-1} + \alpha_{k - 1} \mathbf{p}_{k-1}\)</span>, the Expanding Subspace Minimization theorem states that,</p>
<div class="math">
\begin{align*}
\mathbf{r}_k^T \mathbf{p}_i = 0 \: \forall \: i \in [\![ 0, k - 1 ]\!] \\
\end{align*}
</div>
<p>and <span class="math">\(\mathbf{x}_k\)</span> is the minimizer of:</p>
<div class="math">
\begin{align*}
f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T H \mathbf{x} + c^T \mathbf{x} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\text{over the set } \{\mathbf{x} \mid \mathbf{x} = \mathbf{x}_0 + \mathrm{span}\{\mathbf{p}_0, \mathbf{p}_1,...\mathbf{p}_{k-1}\}\} \\
\end{align*}
</div>
<p>The new residual <span class="math">\(\mathbf{r}_k\)</span> is orthogonal to all the search directions used in the previous iterations (provided that the search directions are H-conjugate}.</p>
<div class="section" id="proof">
<h4>Proof:</h4>
<p>We will show that <span class="math">\(\tilde{\mathbf{x}} = \mathbf{x}_0 + \sigma_0^*\mathbf{p}_0 + \dots + \sigma_{k-1}^*\mathbf{p}_{k-1}\)</span> minimizes <span class="math">\(f(\mathbf{x})\)</span> over <span class="math">\(\{\mathbf{x} \mid \mathbf{x} = \mathbf{x}_0 + \mathrm{span}\{\mathbf{p}_0, \mathbf{p}_1,...\mathbf{p}_{k-1}\}\}\)</span>, if and only if <span class="math">\(\mathbf{r}(\tilde{\mathbf{x}})^T \mathbf{p}_i = 0, \: i = 0,...., k - 1\)</span>.</p>
<p>Let,</p>
<div class="math">
\begin{align*}
h(\sigma) = f(\mathbf{x}_0 + \sigma_0^*\mathbf{p}_0 + \dots + \sigma_{k-1}^*\mathbf{p}_{k-1}) \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\sigma = (\sigma_0, \sigma_1, ..., \sigma_{k-1})^T \\
\end{align*}
</div>
<p><span class="math">\(h(\sigma)\)</span> is a strict convex quadratic. Therefore, at the unique minimiser <span class="math">\(\sigma^*\)</span>,</p>
<div class="math">
\begin{align*}
\frac{\partial h (\sigma^*)}{\partial \sigma_i} = 0 \\
\end{align*}
</div>
<p>By Chain Rule,</p>
<div class="math">
\begin{align*}
\frac{\partial h (\sigma^*)}{\partial \tilde{\mathbf{x}}} \frac{\partial \tilde{\mathbf{x}}}{\partial \sigma_i} = 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\nabla f(\mathbf{x}_0 + \sigma_0^*\mathbf{p}_0 + \dots + \sigma_{k-1}^*\mathbf{p}_{k-1})^T \mathbf{p}_i = 0, \: i = 0,...,k-1 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\nabla f(\tilde{\mathbf{x}})^T \mathbf{p}_i = 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{r}(\tilde{\mathbf{x}})^T \mathbf{p}_i = 0, \: i = 0,..., k-1 \\
\end{align*}
</div>
<p>Hence, from the above, we can conclude that <span class="math">\(\tilde{\mathbf{x}}\)</span> minimizes <span class="math">\(f(\mathbf{x})\)</span> over <span class="math">\(\{\mathbf{x},  \mathbf{x} = \mathbf{x}_0 + \mathrm{span}\{\mathbf{p}_0, \mathbf{p}_1,...\mathbf{p}_{k-1}\}\}\)</span>, by the result that <span class="math">\(\mathbf{r}(\tilde{\mathbf{x}})^T \mathbf{p}_i = 0, \: i = 0,...., k - 1\)</span>.</p>
<p>Now, we will use induction to show that <span class="math">\(\mathbf{x}_k\)</span> satisfies <span class="math">\(\mathbf{r}(\tilde{\mathbf{x}})^T \mathbf{p}_i = 0\)</span>, and therefore, <span class="math">\(\mathbf{x}_k = \tilde{\mathbf{x}}\)</span>.</p>
<p>For <span class="math">\(k = 1\)</span>, <span class="math">\(\mathbf{x}_1 = \mathbf{x}_0 + \alpha_0 \mathbf{p}_0\)</span> minimizes <span class="math">\(f(\mathbf{x})\)</span> along <span class="math">\(\mathbf{p}_0\)</span> which implies <span class="math">\(\mathbf{r}_1^T \mathbf{p}_0 = 0\)</span>.</p>
<p>Let our induction hypothesis hold true for <span class="math">\(\mathbf{r}_{k-1}\)</span>, that is, <span class="math">\(\mathbf{r}_{k-1}^T \mathbf{p}_i = 0, \: \forall \: i = 0, \dots , k-2\)</span>. We will prove the orthogonality relationships for <span class="math">\(\mathbf{r}_{k}\)</span>. We know that,</p>
<div class="math">
\begin{align*}
\mathbf{r}_{k} = \mathbf{r}_{k - 1} + \alpha_{k - 1} H \mathbf{p}_{k - 1} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{p}_{k-1}^T \mathbf{r}_{k} = \mathbf{p}_{k-1}^T \mathbf{r}_{k - 1} +  \alpha_{k - 1} \mathbf{p}_{k - 1}^T H \mathbf{p}_{k - 1} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{r}_{k}^T \mathbf{p}_{k-1}  = 0 \text{ after substituting the value of } \alpha_{k - 1} = - \frac{\mathbf{p}_{k - 1}^T \mathbf{r}_{k - 1}}{\mathbf{p}_{k - 1}^T H \mathbf{p}_{k - 1}} \\
\end{align*}
</div>
<p>For the rest of the vectors <span class="math">\(\mathbf{p}_i, \: i = 0, \dots, k - 2\)</span>,</p>
<div class="math">
\begin{align*}
\mathbf{r}_{k} = \mathbf{r}_{k - 1} + \alpha_{k - 1} H \mathbf{p}_{k - 1} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{p}_{i}^T \mathbf{r}_{k} = \mathbf{p}_{i}^T \mathbf{r}_{k - 1} +  \alpha_{k - 1} \mathbf{p}_{i}^T H \mathbf{p}_{k - 1} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{r}_{k}^T \mathbf{p}_{i} = \mathbf{r}_{k - 1}^T \mathbf{p}_{i} +  \alpha_{k - 1} \mathbf{p}_{k - 1} H \mathbf{p}_{i}^T \\
\end{align*}
</div>
<p>The first term on the Right Hand Side vanishes due to our induction hypothesis. The second term vanishes due to the H-conjugacy of the <span class="math">\(\mathbf{p}_{i}\)</span> vectors. Therefore,</p>
<div class="math">
\begin{align*}
\mathbf{r}_{k}^T \mathbf{p}_{i} = 0 \: \forall \: i \in  [\![ 0, k - 2 ]\!] \\
\end{align*}
</div>
<p>Combining both of the above results, we get,</p>
<div class="math">
\begin{align*}
\mathbf{r}_{k}^T \mathbf{p}_{i} = 0 \: \forall \: i \in  [\![ 0, k - 1 ]\!] \\
\end{align*}
</div>
<p>and therefore, <span class="math">\(\mathbf{x}_k\)</span> is the minimizer of:</p>
<div class="math">
\begin{align*}
f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T H \mathbf{x} + c^T \mathbf{x} \\
\end{align*}
</div>
<p>over <span class="math">\(\{\mathbf{x} \mid \mathbf{x} = \mathbf{x}_0 + \mathrm{span}\{\mathbf{p}_0, \mathbf{p}_1,...\mathbf{p}_{k-1}\}\}\)</span>. Hence, proved.</p>
</div>
</div>
<div class="section" id="connection-to-krylov-subspaces">
<h3>Connection to Krylov Subspaces</h3>
<p>The Expanding Subspace Minimisation (ESM) Theorem proves two things: the next iterate <span class="math">\(\mathbf{x}_i\)</span> minimizes our objective <span class="math">\(f(\mathbf{x})\)</span> over the entire subspace <span class="math">\(\mathbf{x}_0 + \underset{k}{\mathrm{span}}\{ \mathbf{p}_k \}_0^{i - 1}\)</span>; and the next residual <span class="math">\(\mathbf{r}_i\)</span> is orthogonal to all the previous search directions <span class="math">\(\{ \mathbf{p}_k \}_0^{i - 1}\)</span>, that is,</p>
<div class="math">
\begin{align*}
\mathbf{r}_i^T \mathbf{p}_k = 0 \: \forall \: k \in [\![ 0, i - 1 ]\!] \\
\end{align*}
</div>
<p>Let us denote this subspace as <span class="math">\(\mathcal{D}_i = \{ \mathbf{p}_k \}_0^{i - 1}\)</span>.</p>
<p>In other words, the residual <span class="math">\(\mathbf{r}_i\)</span> is orthogonal to the entire subspace <span class="math">\(\mathcal{D}_i\)</span>, and therefore, any vector contained in <span class="math">\(\mathcal{D}_i\)</span> will be orthogonal to <span class="math">\(\mathbf{r}_i\)</span>!</p>
<div class="math">
\begin{align*}
\mathbf{r}_i \perp \mathcal{D}_i \\
\end{align*}
</div>
<p>The image below demonstrates this fact for <span class="math">\(\mathcal{D}_2\)</span>.</p>
<h1 align="center">
<img src="/images/osubs.svg" width="700"/>
</h1><p>Here we attempt to analyse a few more fundamental issues.</p>
<p>In plain English, what the first part of the ESM theorem is saying is that given a starting point <span class="math">\(\mathbf{x}_0\)</span>, we keep on enlarging the <em>expanding-subspace</em> <span class="math">\(\mathcal{D}_i\)</span> by one dimension in each step, and we evermore keep getting closer to the solution after gaining that extra degree of freedom of movement in that step. As soon as <span class="math">\(\mathcal{D}_i\)</span> becomes <span class="math">\(\mathbb{R}^n\)</span>, the error gets driven to zero and we have reached our destination!</p>
<p>Since we have mentioned in the previous section that the directions <span class="math">\(\mathbf{p}\)</span> are built from the linearly independent set of vectors <span class="math">\(\mathbf{u}\)</span>, that is,</p>
<div class="math">
\begin{align*}
\mathbf{p}_i = \mathbf{u}_i + \sum_{k = 0}^{i - 1} \beta_{ik} \mathbf{p}_k \\
\end{align*}
</div>
<p>..it implies that the space spanned by <span class="math">\(\{ \mathbf{p}_k \}_0^{i - 1}\)</span> is the same as the space spanned by <span class="math">\(\{ \mathbf{u}_k \}_0^{i - 1}\)</span> by linearity. That is, <span class="math">\(\mathcal{D}_i = \{ \mathbf{u}_k \}_0^{i - 1}\)</span>. Since, <span class="math">\(\mathbf{r}_i \perp \mathcal{D}_i\)</span>, this means, the next residual <span class="math">\(\mathbf{r}_i\)</span> is also orthogonal to all the previous and current generator <span class="math">\(\{ \mathbf{u}_k \}_0^{i - 1}\)</span> vectors lying in the space <span class="math">\(\mathcal{D}_i\)</span>! Mathematically, by first transposing the above equation,</p>
<div class="math">
\begin{align*}
\mathbf{p}_i^T = \mathbf{u}_i^T + \sum_{k = 0}^{i - 1} \beta_{ik} \mathbf{p}_k^T \\
\end{align*}
</div>
<p>Let <span class="math">\(i &lt; j\)</span> or <span class="math">\(i \in [\![ 0, j - 1 ]\!]\)</span>. Then,</p>
<div class="math">
\begin{align*}
\mathbf{p}_i^T \mathbf{r}_j = \mathbf{u}_i^T \mathbf{r}_j + \sum_{k = 0}^{i - 1} \beta_{ik} \mathbf{p}_k^T \mathbf{r}_j \\
\end{align*}
</div>
<p>Using subspace minimization theorem,</p>
<div class="math">
\begin{align*}
0 = \mathbf{u}_i^T \mathbf{r}_j + 0 \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{r}_j^T \mathbf{u}_i = 0 \: \forall \: i \in [\![ 0, j - 1 ]\!] \\
\end{align*}
</div>
<p>Hence, we see why it is true.</p>
<p>Now, comes the core idea of <strong>Conjugate Gradient</strong>: Instead of using the coordinate axes <span class="math">\(\{ \mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n \}\)</span> as the generator vectors <span class="math">\(\{ \mathbf{u}_0, \mathbf{u}_1, \dots, \mathbf{u}_{n - 1} \}\)</span> for the conjugated-construction of our search directions <span class="math">\(\mathbf{p}_i\)</span>, we will instead use the negative residuals/gradients <span class="math">\(\{ - \mathbf{r}_0, - \mathbf{r}_1, \dots, - \mathbf{r}_{n - 1} \}\)</span> as the generator vectors! That is, <span class="math">\(\mathbf{u}_i = - \mathbf{r}_i\)</span>. This is precisely the reason why we have emphasized on the term, <strong>Conjugate(d) Gradients</strong>. Because the residuals/gradients are themselves not H-conjugate, but the <em>n</em> H-conjugate directions that we need for our algorithm are constructed out of them.</p>
<p>While this choice seems arbitrary, we will also see how this choice leads to the efficiency and some more desirable attributes of the CG algorithm. Since <span class="math">\(\mathbf{p}_0 = \mathbf{u}_0 = -\mathbf{r}_0 = -\nabla f(\mathbf{x}_0)\)</span>, the first direction of the Conjugate Gradient process is also the direction of Steepest Descent.</p>
<p>We stated before that,</p>
<div class="math">
\begin{align*}
\implies \mathbf{r}_j^T \mathbf{u}_i = 0 \: \forall \: i \in [\![ 0, j - 1 ]\!] \\
\end{align*}
</div>
<p>Replace <span class="math">\(\mathbf{u}_i = -\mathbf{r}_i\)</span> in the above and we shall get,</p>
<div class="math">
\begin{align*}
\implies - \mathbf{r}_j^T \mathbf{r}_i = 0 \: \forall \: i \in [\![ 0, j - 1 ]\!] \\
\end{align*}
</div>
<p>(Dropping the negative sign, and replacing the indices <span class="math">\(i\)</span> by <span class="math">\(k\)</span> and <span class="math">\(j\)</span> by <span class="math">\(i\)</span> with no difference)</p>
<div class="math">
\begin{align*}
\implies \mathbf{r}_i^T \mathbf{r}_k = 0  \: \forall \: k \in [\![ 0, i - 1 ]\!] \\
\end{align*}
</div>
<p>Which means the next residual/gradient is orthogonal to all the previous residuals/gradients. Since <span class="math">\(\mathbf{u}_i\)</span> (generator) vectors <strong>have to be linearly independent</strong> for generating our directions, by selecting <span class="math">\(\mathbf{u}_i = -\mathbf{r}_i\)</span> we are not doing any mistake since the mutual orthogonality of <span class="math">\(\mathbf{r}_i \: \forall \: i \in [\![ 0, n - 1 ]\!]\)</span> guarantees their linear independence. Contrast this with Steepest Descent, where a lot of residuals/gradients were repeated directions and hence not linearly independent.</p>
<p>Therefore, the space spanned by directions <span class="math">\(\{ \mathbf{p}_k \}_0^{i - 1}\)</span> is equal to the space spanned by the residuals <span class="math">\(\{ \mathbf{r}_k \}_0^{i - 1}\)</span> (since <span class="math">\(\{ \mathbf{p}_k \}_0^{i - 1} = \{ \mathbf{u}_k \}_0^{i - 1}\)</span>).</p>
<div class="math">
\begin{align*}
\mathcal{D}_i = \underset{k}{\mathrm{span}}\{ \mathbf{p}_k \}_0^{i - 1} = \underset{k}{\mathrm{span}}\{ \mathbf{u}_k \}_0^{i - 1} = \underset{k}{\mathrm{span}}\{ \mathbf{r}_k \}_0^{i - 1} \\
\end{align*}
</div>
<p>Further, let us develop one more statement:</p>
<div class="math">
\begin{align*}
\mathbf{p}_i^T \mathbf{r}_i = \mathbf{u}_i^T \mathbf{r}_i + \sum_{k = 0}^{i - 1} \beta_{ik} \mathbf{p}_k^T \mathbf{r}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{p}_i^T \mathbf{r}_i = \mathbf{u}_i^T \mathbf{r}_i = - \mathbf{r}_i^T \mathbf{r}_i \\
\end{align*}
</div>
<p>We know that,</p>
<div class="math">
\begin{align*}
\mathbf{r}_i = \mathbf{r}_{i - 1} + \alpha_{i - 1} H \mathbf{p}_{i - 1} \\
\end{align*}
</div>
<p>Since, <span class="math">\(\mathbf{r}_i\)</span> lies outside <span class="math">\(\mathcal{D}_i\)</span> from the subspace identity, and <span class="math">\(\mathbf{r}_{i - 1} \in \mathcal{D}_i\)</span>, therefore, the <span class="math">\(H \mathbf{p}_{i - 1}\)</span> vector has to lie outside the space spanned by <span class="math">\(\mathcal{D}_i\)</span>. From this, we can write: <span class="math">\(\mathbf{p}_{i - 1} \in \mathcal{D}_i \implies H \mathbf{p}_{i - 1} \in H \mathcal{D}_i\)</span>. Therefore, each subspace <span class="math">\(\mathcal{D}_{i + 1} = \underset{k}{\mathrm{span}}\{ \{ \mathbf{r}_k \}_0^{i - 1},  \mathbf{r}_i \}\)</span> is constructed from the union of <span class="math">\(\mathcal{D}_i\)</span> and <span class="math">\(H \mathcal{D}_i\)</span>.</p>
<p>Starting from the first subspace,</p>
<div class="math">
\begin{align*}
\mathcal{D}_1 = \text{span}\{ \mathbf{p}_0 \} \\
\end{align*}
</div>
<p>The second subspace is,</p>
<div class="math">
\begin{align*}
\mathcal{D}_2 = \text{span}\{ \mathbf{p}_0 \} + H \text{span}\{ \mathbf{p}_0 \} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathcal{D}_2 = \text{span}\{ \mathbf{p}_0, H \mathbf{p}_0 \} \\
\end{align*}
</div>
<p>The third subspace is,</p>
<div class="math">
\begin{align*}
\mathcal{D}_3 = \text{span}\{ \mathbf{p}_0, H \mathbf{p}_0 \} + H \text{span}\{ \mathbf{p}_0, H \mathbf{p}_0 \} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathcal{D}_3 = \text{span}\{ \mathbf{p}_0, H \mathbf{p}_0, H (H \mathbf{p}_0) \} \\
\end{align*}
</div>
<p>and therefore, by recursion, the i-th subspace is,</p>
<div class="math">
\begin{align*}
\mathcal{D}_i = \text{span}\{ \mathbf{p}_0, H \mathbf{p}_0, H (H \mathbf{p}_0), H (H (H \mathbf{p}_0)), \dots, H^{i - 1} \mathbf{p}_0\} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathcal{D}_i = \text{span}\{ \mathbf{p}_0, H \mathbf{p}_0, H^2 \mathbf{p}_0, H^3 \mathbf{p}_0, \dots, H^{i - 1} \mathbf{p}_0\} \\
\end{align*}
</div>
<p>Since, <span class="math">\(\mathcal{D}_i = \underset{k}{\mathrm{span}}\{ \mathbf{p}_k \}_0^{i - 1} = \underset{k}{\mathrm{span}}\{ \mathbf{r}_k \}_0^{i - 1}\)</span> by our generator choice, by the same arguments,</p>
<div class="math">
\begin{align*}
\implies \mathcal{D}_i = \text{span}\{ \mathbf{r}_0, H \mathbf{r}_0, H^2 \mathbf{r}_0, H^3 \mathbf{r}_0, \dots, H^{i - 1} \mathbf{r}_0\} \\
\end{align*}
</div>
<p>This subspace has a special name: <strong>Krylov Subspace</strong>, denoted by <span class="math">\(\mathcal{K}_i\)</span>. This particular subspace, which we constructed out of choosing the generator vectors <span class="math">\(\mathbf{u}_i\)</span> as the residuals/gradients <span class="math">\(\mathbf{r}_i\)</span>, is in fact the <strong>core property</strong> that makes our Conjugate Gradient method a highly efficient one. So why is that? If we look carefully, we can observe that the Krylov Subspace <span class="math">\(\mathcal{D}_{i + 1}\)</span> is the union of <span class="math">\(\mathcal{D}_i\)</span> and <span class="math">\(H \mathcal{D}_i\)</span> by construction. Since <span class="math">\(\mathbf{r}_{i + 1} \perp \mathcal{D}_{i + 1}\)</span> as proven by the ESM Theorem, we can write,</p>
<div class="math">
\begin{align*}
\mathbf{r}_{i + 1} \perp \mathcal{D}_{i + 1} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{r}_{i + 1} \perp \left( \mathcal{D}_i \bigcup H \mathcal{D}_i \right) \\
\end{align*}
</div>
<p>resulting in,</p>
<div class="math">
\begin{align*}
\mathbf{r}_{i + 1} \perp \mathcal{D}_i \\
\end{align*}
</div>
<p>and,</p>
<div class="math">
\begin{align*}
\mathbf{r}_{i + 1} \perp H \mathcal{D}_i \\
\end{align*}
</div>
<p>which is a funny result. Simplifying the above expression with <span class="math">\(\mathcal{D}_i = \underset{k}{\mathrm{span}}\{ \mathbf{p}_k \}_0^{i - 1} = \mathrm{span}\{ \mathbf{p}_0, \mathbf{p}_1, \mathbf{p}_2, \dots, \mathbf{p}_{i - 1} \}\)</span>,</p>
<div class="math">
\begin{align*}
\mathbf{r}_{i + 1} \perp H \mathrm{span}\{ \mathbf{p}_0, \mathbf{p}_1, \mathbf{p}_2, \dots, \mathbf{p}_{i - 1} \} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{r}_{i + 1} \perp \mathrm{span}\{ H \mathbf{p}_0, H \mathbf{p}_1, H \mathbf{p}_2, \dots, H \mathbf{p}_{i - 1} \} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{r}_{i + 1}^T H \mathbf{p}_k = 0 \: \forall \: k \in [\![ 0, i - 1 ]\!] \\
\end{align*}
</div>
<p>What this is saying is that <span class="math">\(\mathbf{r}_{i + 1}\)</span> is H-conjugate to all the previous search directions except <span class="math">\(\mathbf{p}_i\)</span> (<span class="math">\(\mathbf{r}_{i + 1}\)</span> is also orthogonal to all the previous search directions as shown above, which is the reason this fact seems funny). Recall that we said in the previous section, to construct a new direction <span class="math">\(\mathbf{p}_{i + 1}\)</span>, we want to take a linearly independent generator <span class="math">\(\mathbf{u}_{i + 1}\)</span> and peel off any of its components that are not H-conjugate to the set of directions <span class="math">\(\{ \mathbf{p}_k \}_0^i\)</span>.</p>
<div class="math">
\begin{align*}
\mathbf{p}_{i + 1} = \mathbf{u}_{i + 1} + \sum_{k = 0}^{i} \beta_{(i + 1),k} \mathbf{p}_k \\
\end{align*}
</div>
<p>Since our generator directions are the residuals/gradients, replace <span class="math">\(\mathbf{u}_{i + 1} = - \mathbf{r}_{i + 1}\)</span>,</p>
<div class="math">
\begin{align*}
\mathbf{p}_{i + 1} = - \mathbf{r}_{i + 1} + \sum_{k = 0}^{i} \beta_{(i + 1),k} \mathbf{p}_k \\
\end{align*}
</div>
<p>BUT! We just proved that <span class="math">\(\mathbf{r}_{i + 1}^T H \mathbf{p}_k = 0 \: \forall \: k \in [\![ 0, i - 1 ]\!]\)</span>! It means there are no components in the vector <span class="math">\(\mathbf{r}_{i + 1}\)</span> that are not H-conjugate to the set of directions <span class="math">\(\{ \mathbf{p}_k \}_0^{i - 1}\)</span>, and therefore we do not have to bother about removing such components from <span class="math">\(\mathbf{r}_{i + 1}\)</span> as we have eliminated their presence! The only direction with which <span class="math">\(\mathbf{r}_{i + 1}\)</span> is not H-conjugate is <span class="math">\(\mathbf{p}_i\)</span>. This greatly simplifies our job: we will simply retain the <span class="math">\(\mathbf{p}_i\)</span> term and drop all other terms (as they are unnecessary):</p>
<div class="math">
\begin{align*}
\mathbf{p}_{i + 1} = - \mathbf{r}_{i + 1} + \beta_{(i + 1),i} \mathbf{p}_i \\
\end{align*}
</div>
<p>Now we simply replace <span class="math">\(\beta_{(i + 1),i} = \beta_{i + 1}\)</span> and voila! Out comes our final form of a new H-conjugate direction:</p>
<div class="math">
\begin{align*}
\mathbf{p}_{i + 1} = - \mathbf{r}_{i + 1} + \beta_{i + 1} \mathbf{p}_i \\
\end{align*}
</div>
<p>To calculate <span class="math">\(\beta_{i + 1}\)</span>, transpose the above equation and post-multiply by <span class="math">\(H \mathbf{p}_i\)</span>,</p>
<div class="math">
\begin{align*}
\mathbf{p}_{i + 1}^T H \mathbf{p}_i = - \mathbf{r}_{i + 1}^T H \mathbf{p}_i + \beta_{i + 1} \mathbf{p}_i^T H \mathbf{p}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \beta_{i + 1} = \frac{\mathbf{r}_{i + 1}^T H \mathbf{p}_i}{\mathbf{p}_i^T H \mathbf{p}_i} \\
\end{align*}
</div>
<p>Using <span class="math">\(\mathbf{r}_{i + 1} = \mathbf{r}_{i} + \alpha_{i} H \mathbf{p}_{i}\)</span>,</p>
<div class="math">
\begin{align*}
\mathbf{r}_{i + 1}^T \mathbf{r}_{i + 1} = \mathbf{r}_{i + 1}^T \mathbf{r}_{i} + \alpha_{i} \mathbf{r}_{i + 1}^T H \mathbf{p}_{i} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{r}_{i + 1}^T \mathbf{r}_{i + 1} = \alpha_{i} \mathbf{r}_{i + 1}^T H \mathbf{p}_{i} \\
\end{align*}
</div>
<p>And,</p>
<div class="math">
\begin{align*}
\mathbf{p}_{i}^T \mathbf{r}_{i + 1} = \mathbf{p}_{i}^T \mathbf{r}_{i} + \alpha_{i} \mathbf{p}_{i}^T H \mathbf{p}_{i} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies 0 = - \mathbf{r}_{i}^T \mathbf{r}_{i} + \alpha_{i} \mathbf{p}_{i}^T H \mathbf{p}_{i} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \mathbf{r}_{i}^T \mathbf{r}_{i} = \alpha_{i} \mathbf{p}_{i}^T H \mathbf{p}_{i} \\
\end{align*}
</div>
<p>Substitute in the expression for <span class="math">\(\beta_{i + 1}\)</span>,</p>
<div class="math">
\begin{align*}
\beta_{i + 1} = \frac{\alpha_{i} \mathbf{r}_{i + 1}^T H \mathbf{p}_i}{\alpha_{i} \mathbf{p}_i^T H \mathbf{p}_i} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\implies \beta_{i + 1} = \frac{\mathbf{r}_{i + 1}^T \mathbf{r}_{i + 1}}{\mathbf{r}_{i}^T \mathbf{r}_{i}} \\
\end{align*}
</div>
<p>Now we are onto something! We neither need to store all directions in memory nor need them to calculate each new direction. This discovery firmly made the CG algorithm one of the mainstays of modern scientific computing.</p>
<p>As as final step, let us prove one more important thing: that the direction <span class="math">\(\mathbf{p}_{i + 1}\)</span> generated by just <span class="math">\(\mathbf{r}_{i + 1}\)</span> and <span class="math">\(\mathbf{p}_i\)</span> at the i-th step is indeed H-conjugate to the entire set <span class="math">\(\{ \mathbf{p}_k \}_0^{i - 1}\)</span> not considered in the formula. Assume that the set upto to the i-th iterate is already H-conjugate.</p>
<p>First, we transpose the following equation:</p>
<div class="math">
\begin{align*}
\mathbf{p}_{i + 1} = - \mathbf{r}_{i + 1} + \beta_{i + 1} \mathbf{p}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{p}_{i + 1}^T = - \mathbf{r}_{i + 1}^T + \beta_{i + 1} \mathbf{p}_i^T \\
\end{align*}
</div>
<p>Post-multiply with <span class="math">\(H \mathbf{p}_{k}\)</span>, <span class="math">\(k \in [\![ 0, i - 1 ]\!]\)</span>,</p>
<div class="math">
\begin{align*}
\mathbf{p}_{i + 1}^T H \mathbf{p}_{k} = - \mathbf{r}_{i + 1}^T H \mathbf{p}_{k} + \beta_{i + 1} \mathbf{p}_i^T H \mathbf{p}_{k} \\
\end{align*}
</div>
<p>Using <span class="math">\(\mathbf{r}_{i + 1}^T H \mathbf{p}_k = 0 \: \forall \: k \in [\![ 0, i - 1 ]\!]\)</span> and the fact that the set <span class="math">\(\{ \mathbf{p}_k \}_0^{i}\)</span> is H-conjugate, both the right hand terms vanish. Therefore,</p>
<div class="math">
\begin{align*}
\mathbf{p}_{i + 1}^T H \mathbf{p}_{k} = 0 \: \forall \: k \in [\![ 0, i - 1 ]\!] \\
\end{align*}
</div>
<p>Hence, proved.</p>
<p>What we have done so far achieves both goal #2 and goal #1. Before presenting the Conjugate Gradient algorithm in the next section, let us summarise the most important facts from this section:</p>
<div class="math">
\begin{align*}
\mathbf{r}_i^T \mathbf{p}_k = 0 \: \forall \: k \in [\![ 0, i - 1 ]\!] \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{r}_i^T \mathbf{r}_k = 0  \: \forall \: k \in [\![ 0, i - 1 ]\!] \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{p}_{i}^T H \mathbf{p}_{k} = 0 \: \forall \: k \in [\![ 0, i - 1 ]\!] \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathbf{p}_i^T \mathbf{r}_i = \mathbf{u}_i^T \mathbf{r}_i = - \mathbf{r}_i^T \mathbf{r}_i \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\mathcal{D}_i = \mathcal{K}_i = \mathrm{span}\{ \mathbf{r}_0, \mathbf{r}_1, \mathbf{r}_2, \dots, \mathbf{r}_{i - 1} \} = \mathrm{span}\{ \mathbf{p}_0, \mathbf{p}_1, \mathbf{p}_2, \dots, \mathbf{p}_{i - 1} \} \\
= \text{span}\{ \mathbf{r}_0, H \mathbf{r}_0, H^2 \mathbf{r}_0, H^3 \mathbf{r}_0, \dots, H^{i - 1} \mathbf{r}_0\} \\
\end{align*}
</div>
</div>
</div>
<div class="section" id="conjugate-gradients-a-method-of-conjugate-directions">
<h2>Conjugate Gradients: A Method of Conjugate Directions</h2>
<p>All of our machinery is in place now. We collect the necessary facts in an iterative loop; the simplicity and terseness of the algorithm completely belies the conceptual buildup we went through in the previous sections. So, without any further ado, let's present the Conjugate Gradient algorithm in one piece now.</p>
<blockquote>
<ol class="arabic simple">
<li>Given, <span class="math">\(\mathbf{x}_0\)</span>.</li>
<li>Set <span class="math">\(\mathbf{r}_0 \leftarrow H \mathbf{x}_0 + \mathbf{c}\)</span>, <span class="math">\(\mathbf{p}_0 \leftarrow - \mathbf{r}_0\)</span> and <span class="math">\(i \leftarrow 0\)</span>.</li>
<li><strong>while</strong> <span class="math">\(\lVert \mathbf{r}_i \rVert_2 \neq 0\)</span>, do:</li>
<li><em>Step Length</em>:</li>
</ol>
<div class="math">
\begin{align*}
\alpha_i = \frac{\mathbf{r}_i^T \mathbf{r}_i}{\mathbf{p}_i^T H \mathbf{p}_i} \\
\end{align*}
</div>
<ol class="arabic simple" start="5">
<li><em>Next Iterate</em>:</li>
</ol>
<div class="math">
\begin{align*}
\mathbf{x}_{i + 1} = \mathbf{x}_i + \alpha_i \mathbf{p}_i \\
\end{align*}
</div>
<ol class="arabic simple" start="6">
<li><em>Next Residual</em>:</li>
</ol>
<div class="math">
\begin{align*}
\mathbf{r}_{i + 1} = \mathbf{r}_i + \alpha_i H \mathbf{p}_i \\
\end{align*}
</div>
<ol class="arabic simple" start="7">
<li><em>Direction Update Factor</em>:</li>
</ol>
<div class="math">
\begin{align*}
\beta_{i + 1} = \frac{\mathbf{r}_{i + 1}^T \mathbf{r}_{i + 1}}{\mathbf{r}_{i}^T \mathbf{r}_{i}} \\
\end{align*}
</div>
<ol class="arabic simple" start="8">
<li><em>Next Direction</em>:</li>
</ol>
<div class="math">
\begin{align*}
\mathbf{p}_{i + 1} = - \mathbf{r}_{i + 1} + \beta_{i + 1} \mathbf{p}_i \\
\end{align*}
</div>
<ol class="arabic simple" start="9">
<li><em>Loop Counter Update</em>:</li>
</ol>
<div class="math">
\begin{align*}
i \leftarrow i + 1 \\
\end{align*}
</div>
<ol class="arabic simple" start="10">
<li><strong>end while</strong></li>
</ol>
</blockquote>
</div>
<div class="section" id="computational-complexity">
<h2>Computational Complexity:</h2>
<p>The dominant costs of the above algorithm is in the Matrix-Vector multiplication step: <span class="math">\(H \mathbf{p}_i\)</span>.</p>
<p>For a dense matrix <span class="math">\(H\)</span>, the Matrix-Vector multiplication costs, with <span class="math">\(H \rightarrow n \times n\)</span> and <span class="math">\(\mathbf{p}_i \rightarrow n \times 1\)</span>, <span class="math">\(\mathcal{O}(n^2)\)</span>.</p>
<p>For a sparse matrix <span class="math">\(H\)</span>, Matrix-Vector multiplication costs, <span class="math">\(\mathcal{O}(m)\)</span>, where <span class="math">\(m\)</span> is the non-zero elements in <span class="math">\(H\)</span>.</p>
<p>If we want the initial residual <span class="math">\(H \mathbf{r}_0\)</span> to decrease by a factor <span class="math">\(\epsilon\)</span>, say:</p>
<p><span class="math">\(\| \mathbf{r}_k\| \leq \epsilon \| \mathbf{r}_0 \|\)</span>, where <span class="math">\(\mathbf{r}_k\)</span> and <span class="math">\(\mathbf{r}_0\)</span> are the residuals in the <span class="math">\(k^{th}\)</span> and the <span class="math">\(0^{th}\)</span> iterations, then the <em>Number of Iterations</em>, <span class="math">\(n_{iter}\)</span> required for the following algorithms are:</p>
<p>Steepest Descent: <span class="math">\(n_{iter} \leq \frac{1}{2}\mathcal{K}\ln{\frac{1}{\epsilon}}\)</span>.</p>
<p>Conjugate Gradient <span class="math">\(n_{iter} \leq \frac{1}{2}\sqrt{\mathcal{K}}\ln{\frac{2}{\epsilon}}\)</span>.</p>
<p>where,</p>
<p><span class="math">\(\mathcal{K}\)</span> is the condition number of <span class="math">\(H\)</span>.</p>
<p>If we have a <span class="math">\(d\)</span>-dimensional domain, to solve the boundary value problem, the condition number <span class="math">\(\mathcal{K}\)</span> of the matrix, obtained by discretizing the second order elliptic boundary value problem, is of the order <span class="math">\(\mathcal{O}(n^\frac{2}{d})\)</span>, and the number of non-zero entries is of <span class="math">\(\mathcal{O}(n)\)</span>.</p>
<p>The Time Complexities for the two methods in this problem are as:</p>
<p>Steepest Descent: <span class="math">\(\mathcal{O}(m\mathcal{K})\)</span></p>
<p>Conjugate Gradient:  <span class="math">\(\mathcal{O}(m\sqrt{\mathcal{K}})\)</span></p>
<p>For a 2D problem and <span class="math">\(m\)</span> <span class="math">\(\epsilon\)</span> <span class="math">\(\mathcal{O}(n)\)</span>, we have the following:</p>
<p>S.D: Time complexity: <span class="math">\(\mathcal{O}(n^2)\)</span></p>
<p>C.G: Time complexity: <span class="math">\(\mathcal{O}(n^\frac{3}{2})\)</span></p>
<p>Both methods have a space complexity of <span class="math">\(\mathcal{O}(m)\)</span>.</p>
</div>
<div class="section" id="convergence-of-cg">
<h2>Convergence of CG:</h2>
<p>In exact arithmetic, the conjugate gradient method will terminate at the solution in at most <span class="math">\(n\)</span> iterations. When the distribution of the eigenvalues of <span class="math">\(H\)</span> has certain favourable features, the algorithm will identify the solution in many fewer than <span class="math">\(n\)</span> iterations.</p>
<p>If <span class="math">\(H\)</span> has only <span class="math">\(r\)</span> distinct eigenvalues, then the CG iteration will terminate at the solution in at most <span class="math">\(r\)</span> iterations.</p>
<p>It is generally true that if the eigenvalues occur in <span class="math">\(r\)</span> distinct clusters, the CG iterates will approximately solve the problem in about <span class="math">\(r\)</span> steps.</p>
</div>
<div class="section" id="pre-conditioned-cg">
<h2>Pre conditioned CG:</h2>
<p>When the condition number <span class="math">\(\mathcal{K}\)</span> of <span class="math">\(H\)</span> is really high, then we run into issues. We introduce a technique called as pre-conditioning.</p>
<p>Here we transform the problem <span class="math">\(Hx = -c\)</span> with the inverse of <span class="math">\(M\)</span> where <span class="math">\(M\)</span> is an SPD and invertible matrix, chosen by the user depending on the problem at hand:</p>
<div class="math">
\begin{align*}
M^{-1} H \mathbf{x} = -M^{-1} \mathbf{c} \\
\end{align*}
</div>
<p>We need: <span class="math">\(\mathcal{K}(M^{-1}H) \ll \mathcal{K}(H)\)</span> or the eigenvalues of <span class="math">\(M^{-1}H\)</span> are clustered together, thereby ensuring faster convergence to the actual solution in <span class="math">\(r\)</span> steps, if there are <span class="math">\(r\)</span> clusters.</p>
<p>Most of the standard solver packages use pre-conditioning for faster convergence.</p>
</div>
<div class="section" id="naive-code-implementation">
<h2>Naive Code Implementation</h2>
<p>The following class implements the reference Conjugate Gradient algorithm without preconditioning, for educational purposes only. Do not use it on your actual code!</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">quadratic_form</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Solve a linear system Hx = -c using a quadratic formulation.</span>
<span class="sd">    Uses the Conjugate Gradient method.</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">H</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">rhs_sign</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""</span>
<span class="sd">        __init__: initialise the CG class.</span>

<span class="sd">        Input data = H, x, c.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        H : np.ndarray</span>
<span class="sd">            System Matrix.</span>
<span class="sd">        c : np.ndarray</span>
<span class="sd">            rhs</span>
<span class="sd">        rhs_sign : np.int64</span>
<span class="sd">            Sign of rhs. If system is Hx = c, set rhs_sign = 1, else if Hx = - c, set rhs_sign = -1.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">H</span> <span class="o">=</span> <span class="n">H</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">res_sgn</span> <span class="o">=</span> <span class="o">-</span><span class="n">rhs_sign</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rows</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cols</span> <span class="o">=</span> <span class="n">H</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">def</span> <span class="nf">cgmin</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x0</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">maxiter</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">tol</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span> <span class="o">=</span> <span class="mf">1.0e-8</span><span class="p">,</span> <span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""</span>
<span class="sd">        cgmin: Minimize positive definite quadratic form with conjugate gradient method.</span>

<span class="sd">        This routine uses the un-preconditioned efficient version of CG.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x0 : np.ndarray</span>
<span class="sd">            Initial Guess Vector</span>

<span class="sd">        maxiter : np.int64</span>
<span class="sd">            Maximum number of allowed iterations</span>

<span class="sd">        tol: np.float64</span>
<span class="sd">            Tolerance</span>

<span class="sd">        debug : bool</span>
<span class="sd">            Print iteration steps</span>
<span class="sd">        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>  <span class="c1"># initial guess</span>
        <span class="n">r</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span> <span class="o">@</span> <span class="n">x0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">res_sgn</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span>  <span class="c1"># initial residual</span>
        <span class="n">p</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="o">-</span><span class="n">r</span>  <span class="c1"># initial direction</span>
        <span class="n">i</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">rtr</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">r</span>
        <span class="k">while</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">tol</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">maxiter</span><span class="p">:</span>
            <span class="n">Hpi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span> <span class="o">@</span> <span class="n">p</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">rtr</span> <span class="o">/</span> <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Hpi</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">p</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">Hpi</span>
            <span class="n">rnr</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">r</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">rnr</span> <span class="o">/</span> <span class="n">rtr</span>
            <span class="n">rtr</span> <span class="o">=</span> <span class="n">rnr</span>
            <span class="n">p</span> <span class="o">=</span> <span class="o">-</span><span class="n">r</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">p</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">debug</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"x is </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2"> at iteration #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">maxiter</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Max Iterations reached. Terminating procedure..."</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error norm attained: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Solution reached in </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> iterations."</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Final Error Norm at solution: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>
<p>The following sample code uses the above class to solve a symmetric positive-definite linear system:</p>
<div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="mi">500</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span>
<span class="n">c</span> <span class="o">=</span> <span class="mi">500</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">qform</span> <span class="o">=</span> <span class="n">quadratic_form</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">qform</span><span class="o">.</span><span class="n">cgmin</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Solution is :"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="available-software">
<h2>Available Software</h2>
<p>A high-quality, expert-developed Python implementation of the Conjugate Gradient method is available in the Scipy library as the <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.cg.html">scipy.sparse.linalg.cg</a> function. See the documentation for more details.</p>
<p>Well engineered, high performance C language implementations of Conjugate Gradient are available in the Intel Math Kernel Library. The following links contain more information:</p>
<blockquote>
<ol class="arabic simple">
<li><a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/onemkl-developer-reference-c/top/sparse-solver-routines/iterative-sparse-solvers-based-on-reverse-communication-interface-rci-iss.html#iterative-sparse-solvers-based-on-reverse-communication-interface-rci-iss_TBL8-4">Iterative Sparse Solvers based on Reverse Communication Interface</a></li>
<li><a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/onemkl-developer-reference-c/top/sparse-solver-routines/iterative-sparse-solvers-based-on-reverse-communication-interface-rci-iss/cg-interface-description.html">CG Interface Description</a></li>
<li><a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/onemkl-developer-reference-c/top/sparse-solver-routines/iterative-sparse-solvers-based-on-reverse-communication-interface-rci-iss/rci-iss-routines/dcg-init.html#dcg-init">dcg_init</a></li>
<li><a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/onemkl-developer-reference-c/top/sparse-solver-routines/iterative-sparse-solvers-based-on-reverse-communication-interface-rci-iss/rci-iss-routines/dcg-check.html#dcg-check">dcg_check</a></li>
<li><a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/onemkl-developer-reference-c/top/sparse-solver-routines/iterative-sparse-solvers-based-on-reverse-communication-interface-rci-iss/rci-iss-routines/dcg.html#dcg">dcg</a></li>
<li><a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/onemkl-developer-reference-c/top/sparse-solver-routines/iterative-sparse-solvers-based-on-reverse-communication-interface-rci-iss/rci-iss-routines/dcg-get.html#dcg-get">dcg_get</a></li>
<li><a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/onemkl-developer-reference-c/top/sparse-solver-routines/preconditioners-based-on-incomplete-lu-factorization-technique.html#preconditioners-based-on-incomplete-lu-factorization-technique">Preconditioners based on Incomplete LU Factorization Technique</a></li>
</ol>
</blockquote>
</div>
<div class="section" id="non-linear-cg">
<h2>Non-Linear CG</h2>
<p>If our objective function is not convex quadratic as we were dealing with till now, we will have to adapt our current way to fit it into a non-linear functions.</p>
<p>We were solving exactly for <span class="math">\(\alpha_k\)</span> till now, but in the nonlinear case, we have to use the Backtracking line-search scheme to calculate <span class="math">\(\alpha\)</span>.</p>
<p>Instead of the residual <span class="math">\(r_k\)</span>, we will use <span class="math">\(\nabla f_k\)</span>, the gradient of the objective function at the current iterate.</p>
<p><span class="math">\(\beta_{k+1}\)</span> has to be recalculated in terms of <span class="math">\(\nabla f_k\)</span> and the new search direction <span class="math">\(p_k\)</span>.</p>
<p><span class="math">\(\beta_{k}\)</span> can be calculated by the following update equations:</p>
<p>Fletcher - Reeves Method</p>
<div class="math">
\begin{align*}
\beta_{FR}^k = \frac{\nabla f_k^T \nabla f_k}{\nabla f_{k-1}^T \nabla f_{k-1}} \\
\end{align*}
</div>
<p>Polak Ribere Method:</p>
<div class="math">
\begin{align*}
\beta_{PR}^k = \frac{\nabla f_k^T (\nabla f_k - \nabla f_{k-1})}{\nabla f_{k-1}^T \nabla f_{k-1}} \\
\end{align*}
</div>
<p>Hestenes-Steifel Method:</p>
<div class="math">
\begin{align*}
\beta_{HS}^k = \frac{\nabla f_k^T (\nabla f_k - \nabla f_{k-1})}{(\nabla f_k - \nabla f_{k-1})^T p_{k-1}}\\
\end{align*}
</div>
</div>
<div class="section" id="references">
<h2>References</h2>
<blockquote>
<ol class="arabic simple">
<li><a class="reference external" href="http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
<li><a class="reference external" href="https://link.springer.com/book/10.1007/978-0-387-40065-5">Nocedal and Wright, Numerical Optimization</a></li>
<li><a class="reference external" href="http://people.maths.ox.ac.uk/~trefethen/text.html">Trefethen and Bau, Numerical Linear Algebra</a></li>
<li><a class="reference external" href="https://math.berkeley.edu/~yonah/files/Linear%20Algebra.pdf">David C. Lay, Linear Algebra and Its Applications</a></li>
<li><a class="reference external" href="http://math.mit.edu/~gs/linearalgebra/">Gilbert Strang, Introduction to Linear Algebra</a></li>
<li><a class="reference external" href="https://www.mheducation.com/highered/product/numerical-methods-engineers-chapra-canale/M9780073397924.html">Chapra and Canale, Numerical Methods for Engineers</a></li>
<li><a class="reference external" href="http://komarix.org/ac/papers/thesis/thesis_html/node10.html">The Wrong Iterative Method: Steepest Descent</a></li>
</ol>
</blockquote>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script> </article>
</aside><!-- /#featured -->
<section class="body" id="extras">
</section><!-- /#extras -->
<footer class="body" id="contentinfo">
<address class="vcard body" id="about">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->
<p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
</footer><!-- /#contentinfo -->
</body>
</html>