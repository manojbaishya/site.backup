<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Pelican" name="generator"/>
<title>2nd CG Article</title>
<link href="https://manojbaishya.github.io/theme/css/main.css" rel="stylesheet"/>
<meta content="We saw how optimizing along conjugate directions helps in ariving at the solution faster. {Pi}, PiTHPj = diagonal Given an H, there does exists..." name="description"/>
</head>
<body class="home" id="index">
<header class="body" id="banner">
<h1><a href="https://manojbaishya.github.io/">Duality Blog</a></h1>
<nav><ul>
<li><a href="https://manojbaishya.github.io/pages/about.html">About</a></li>
<li class="active"><a href="https://manojbaishya.github.io/category/articles.html">articles</a></li>
<li><a href="https://manojbaishya.github.io/category/linear-algebra.html">linear-algebra</a></li>
</ul></nav>
</header><!-- /#banner -->
<section class="body" id="content">
<article>
<header>
<h1 class="entry-title">
<a href="https://manojbaishya.github.io/2nd-cg-article.html" rel="bookmark" title="Permalink to 2nd CG Article">2nd CG Article</a></h1>
</header>
<div class="entry-content">
<footer class="post-info">
<abbr class="published" title="2021-01-22T13:20:00+05:30">
                Published: Fri 22 January 2021
        </abbr>
<address class="vcard author">
                By                         <a class="url fn" href="https://manojbaishya.github.io/author/manoj-baishya.html">Manoj Baishya</a>
</address>
<p>In <a href="https://manojbaishya.github.io/category/articles.html">articles</a>.</p>
</footer><!-- /.post-info --> <p>We saw how optimizing along conjugate directions helps in ariving at the solution faster.</p>
<p><span class="math inline">{<em>P</em><sub><em>i</em></sub>}, <em>P</em><sub><em>i</em></sub><sup><em>T</em></sup><em>H</em><em>P</em><sup><em>j</em></sup> = <em>d</em><em>i</em><em>a</em><em>g</em><em>o</em><em>n</em><em>a</em><em>l</em></span></p>
<p>Given an H, there does exists these H conjugate directions. For a set <span class="math inline"><em>P</em><sub><em>i</em></sub></span>, it forms a linearly independent set or basis in <span class="math inline">ℛ<sup><em>n</em></sup></span>.</p>
<h2 id="at-the-kth-iteration-we-will-search-along-the-dorection-p_k"><strong>1. At the kth iteration, we will search along the dorection <span class="math inline"><em>p</em><sub><em>k</em></sub></span></strong>:</h2>
<blockquote>
<p>$ x_{k+1} = x_k + _k p_k$ <span class="math inline"><em>α</em><sub><em>k</em></sub> = ?</span> <span class="math inline">$\phi(\alpha) = \frac{1}{2}\alpha_kP_k^THP_k\alpha_k + c^T\alpha_kP_k + constant$</span> <span class="math inline">$\frac{\partial \phi}{\partial \alpha_k} = 0$</span> $_k =  $</p>
</blockquote>
<blockquote>
<p>Let’s define the residual term at the kth iteration now: <span class="math inline"><em>r</em><sub><em>k</em></sub> = <em>H</em><em>x</em><sub><em>k</em></sub> + <em>c</em></span> <span class="math inline">$\alpha_k = \frac{-r_k^TP_k}{P_k^THP_k}$</span></p>
</blockquote>
<h2 id="residal-r_k"><strong>2. Residal <span class="math inline"><em>r</em><sub><em>k</em></sub></span></strong>:</h2>
<blockquote>
<p>$ x_{k+1} = x_k + _k P_k$ <span class="math inline"><em>H</em><em>x</em><sub><em>k</em> + 1</sub> = <em>H</em><em>x</em><sub><em>k</em></sub> + <em>α</em><sub><em>k</em></sub><em>H</em><em>P</em><sub><em>k</em></sub></span> <span class="math inline"><em>H</em><em>x</em><sub><em>k</em> + 1</sub> + <em>c</em> = <em>H</em><em>x</em><sub><em>k</em></sub> + <em>c</em> + <em>α</em><sub><em>k</em></sub><em>H</em><em>P</em><sub><em>k</em></sub></span> <span class="math inline"><em>r</em><sub><em>k</em> + 1</sub> = <em>r</em><sub><em>k</em></sub> + <em>α</em><sub><em>k</em></sub><em>H</em><em>P</em><sub><em>k</em></sub></span></p>
</blockquote>
<h2 id="expanding-sunbspace-minimization-theorem"><strong>3. Expanding sunbspace Minimization theorem:</strong></h2>
<blockquote>
<p>Let <span class="math inline"><em>x</em><sub>0</sub> ∈ ℛ<sup><em>n</em></sup></span>. Given <span class="math inline"><em>P</em><sub><em>K</em></sub></span>, which are H conjugate up until the kth iteration, we will generate <span class="math inline"><em>x</em><sub><em>k</em></sub></span>, and the seaquence <span class="math inline"><em>x</em><sub><em>k</em></sub> = <em>x</em><sub><em>k</em> − 1</sub> + <em>α</em><sub><em>k</em> − 1</sub><em>p</em><sub><em>k</em> − 1</sub></span> is generates using the above update equation.</p>
</blockquote>
<blockquote>
<p>Then: <span class="math inline"><em>r</em><sub><em>k</em></sub><sup><em>T</em></sup><em>p</em><sub><em>i</em></sub> = 0, <em>i</em> = 0, ...<em>k</em> − 1</span> and <span class="math inline"><em>x</em><sub><em>k</em></sub></span> is the minimizer of <span class="math inline">$\phi(x) = \frac{1}{2}x^THx + c^Tx$</span> over the set <span class="math inline">{<em>x</em>, <em>x</em> = <em>x</em><sub>0</sub> + <em>s</em><em>p</em><em>a</em><em>n</em>{<em>p</em><sub>0</sub>, <em>p</em><sub>1</sub>, ...<em>p</em><sub><em>k</em> − 1</sub>}}</span>. The new residual <span class="math inline"><em>r</em><sub><em>k</em></sub></span> is going to be orthogonal to every other search direction used in the previous iterstions (provided that the search directions are H-conjugate}.</p>
</blockquote>
<blockquote>
<p>Proof:</p>
</blockquote>
<ol type="1">
<li>We will show that <span class="math inline"><em>x̃</em></span> minimizes <span class="math inline"><em>ϕ</em>(<em>x</em>)</span> over <span class="math inline">{<em>x</em>, <em>x</em> = <em>x</em><sub>0</sub> + <em>s</em><em>p</em><em>a</em><em>n</em>{<em>p</em><sub>0</sub>, <em>p</em><sub>1</sub>, ...<em>p</em><sub><em>k</em> − 1</sub>}}</span>, iff <span class="math inline"><em>r</em>(<em>x̃</em>)<sup><em>T</em></sup><em>p</em><sub><em>i</em></sub> = 0, <em>i</em> = 0, ...., <em>k</em> − 1.</span></li>
</ol>
<blockquote>
<blockquote>
<p><span class="math inline"><em>σ</em> = (<em>σ</em><sub>0</sub>, <em>σ</em><sub>1</sub>, ..., <em>σ</em><sub><em>k</em> − 1</sub>)<sup><em>T</em></sup></span> <span class="math inline"><em>h</em>(<em>σ</em>)</span> is a strict convex quadratic. <span class="math inline">$\frac{\partial h}{\partial \sigma_i} = 0$</span> <span class="math inline">∇<em>ϕ</em>(<em>x</em><sub>0</sub> + <em>σ</em><sub>0</sub><sup>*</sup><em>p</em><sub>0</sub> + ..... + <em>σ</em><sub><em>k</em> − 1</sub><sup>*</sup><em>p</em><sub><em>k</em> − 1</sub>)<sup><em>T</em></sup><em>p</em><sub><em>i</em></sub> = 0, <em>i</em> = 0, ..., <em>k</em> − 1</span> <span class="math inline"><em>x̃</em> = <em>x</em><sub>0</sub> + <em>σ</em><sub>0</sub><sup>*</sup><em>p</em><sub>0</sub> + ..... + <em>σ</em><sub><em>k</em> − 1</sub><sup>*</sup><em>p</em><sub><em>k</em> − 1</sub></span> <span class="math inline">∇<em>ϕ</em>(<em>x̃</em>)<sup><em>T</em></sup><em>p</em><sub><em>i</em></sub> = 0</span> <span class="math inline"><em>r</em>(<em>x̃</em>)<sup><em>T</em></sup><em>p</em><sub><em>i</em></sub> = 0, <em>i</em> = 0, ..., <em>k</em> − 1</span> We will use induction to show that <span class="math inline"><em>x</em><sub><em>k</em></sub></span> satisfies <span class="math inline"><em>r</em><sub><em>k</em></sub><sup><em>T</em></sup><em>p</em><sub><em>i</em></sub> = 0</span>. For k = 1, <span class="math inline"><em>x</em><sub>1</sub> = <em>x</em><sub>0</sub> + <em>α</em><sub>0</sub><em>p</em><sub>0</sub></span> Induction Hypothesis: <span class="math inline"><em>r</em><sub><em>k</em> − 1</sub><sup><em>T</em></sup><em>p</em><sub><em>i</em></sub> = 0, <em>i</em> = 0, ..., <em>k</em> − 2</span> <span class="math inline"><em>r</em><sub><em>k</em></sub> = <em>r</em><sub><em>k</em> − 1</sub> + <em>α</em><sub><em>k</em> − 1</sub><em>H</em><em>p</em><sub><em>k</em> − 1</sub></span> <span class="math inline"><em>p</em><sub><em>k</em> − 1</sub><sup><em>T</em></sup><em>r</em><sub><em>k</em></sub> = <em>p</em><sub><em>k</em> − 1</sub><sup><em>T</em></sup><em>r</em><sub><em>k</em> − 1</sub> + <em>p</em><sub><em>k</em> − 1</sub><sup><em>T</em></sup><em>α</em><sub><em>k</em> − 1</sub><em>H</em><em>p</em><sub><em>k</em> − 1</sub></span> <span class="math inline"><em>p</em><sub><em>k</em> − 1</sub><sup><em>T</em></sup><em>r</em><sub><em>k</em></sub> = 0</span> For other vectors <span class="math inline"><em>p</em><sub><em>i</em></sub>, <em>i</em> = 0, ..., <em>k</em> − 2,</span> <span class="math inline"><em>p</em><sub><em>i</em></sub><sup><em>T</em></sup><em>r</em><sub><em>k</em></sub> = <em>p</em><sub><em>i</em></sub><sup><em>T</em></sup><em>r</em><sub><em>k</em> − 1</sub> + <em>p</em><sub><em>i</em></sub><sup><em>T</em></sup><em>α</em><sub><em>k</em> − 1</sub><em>H</em><em>p</em><sub><em>k</em> − 1</sub></span> <span class="math inline"><em>p</em><sub><em>i</em></sub><sup><em>T</em></sup><em>r</em><sub><em>k</em></sub> = 0</span> <span class="math inline"><em>r</em><sub><em>k</em></sub><sup><em>T</em></sup><em>p</em><sub><em>i</em></sub> = 0, <em>i</em> = 0, ..., <em>k</em> − 1</span></p>
</blockquote>
</blockquote>
<h2 id="how-to-get-the-conjugate-directions"><strong>4. How to get the conjugate directions:</strong></h2>
<blockquote>
<ol type="a">
<li>Eigenvectors of the Hessian matrix:</li>
</ol>
<blockquote>
<p>Computationally very expensive to do. For large problems, this might be a bottleneck. Time and space complexity is very very high.</p>
</blockquote>
</blockquote>
<blockquote>
<ol start="2" type="a">
<li>Gram-Schmidt Orthogonalization:</li>
</ol>
<blockquote>
<p><span class="math inline"><em>P</em><sup><em>T</em></sup><em>P</em> = <em>I</em></span> Here we need: <span class="math inline"><em>P</em><sup><em>T</em></sup><em>H</em><em>P</em> = <em>D</em><em>i</em><em>a</em><em>g</em><em>o</em><em>n</em><em>a</em><em>l</em></span></p>
</blockquote>
</blockquote>
<p>Still not preferred, as we need to store all the directions, and only after the end of the algorithm, we will get the respective directions.</p>
<p>So we will take the knowledge that we have from the line search methods to solve this problem.</p>
<h2 id="conjugate-gradient-method"><strong>5. Conjugate Gradient Method:</strong></h2>
<blockquote>
<p>Calculate <span class="math inline"><em>p</em><sub><em>k</em></sub></span> only from <span class="math inline"><em>p</em><sub><em>k</em> − 1</sub></span> and <span class="math inline">∇<em>f</em><sub><em>k</em></sub> = <em>r</em><sub><em>k</em></sub></span> <span class="math inline"><em>p</em><sub><em>k</em></sub> =  − <em>r</em><sub><em>k</em></sub> + <em>β</em><sub><em>k</em></sub><em>p</em><sub><em>k</em> − 1</sub></span> We define this new update scheme, and we set <span class="math inline"><em>β</em><sub><em>k</em></sub></span> such that <span class="math inline"><em>p</em><sub><em>k</em> − 1</sub><sup><em>T</em></sup><em>H</em><em>p</em><sub><em>k</em></sub> = 0</span> We are using the direction of <span class="math inline"><em>r</em><sub><em>k</em></sub></span> and the previous search direction, in finding the new direction, in such a way that the new direction <span class="math inline"><em>p</em><sub><em>k</em></sub></span> is H-conjugate with the previous directions. <span class="math inline"><em>p</em><sub><em>k</em> − 1</sub><sup><em>T</em></sup><em>H</em><em>p</em><sub><em>k</em></sub> =  − <em>p</em><sub><em>k</em> − 1</sub><sup><em>T</em></sup><em>H</em><em>r</em><sub><em>k</em></sub> + <em>β</em><sub><em>k</em></sub><em>p</em><sub><em>k</em> − 1</sub><sup><em>T</em></sup><em>H</em><em>p</em><sub><em>k</em> − 1</sub></span> <span class="math inline">$\beta_k = \frac {p_{k-1}^THr_k}{p_{k-1}^THp_{k-1}} = \frac {r_k^THp_{k-1}}{p_{k-1}^THp_{k-1}}$</span> We can see that if we have an iterative method to calculate <span class="math inline"><em>H</em><em>p</em><sub><em>k</em> − 1</sub></span>, that is enough.</p>
</blockquote>
<h3 id="conjugate-gradient-algorithm"><strong>Conjugate Gradient Algorithm:</strong></h3>
<blockquote>
<p>The first direction to search for is the steepest descent direction. <span class="math inline"><em>p</em><sub>0</sub> =  − ∇<em>f</em><sub>0</sub> =  − <em>r</em><sub>0</sub></span>, Steepest descent direction. Given <span class="math inline"><em>x</em><sub>0</sub></span>, Set <span class="math inline"><em>r</em><sub>0</sub> = <em>H</em><em>x</em><sub>0</sub> + <em>c</em>; <em>p</em><sub>0</sub> =  − <em>r</em><sub>0</sub>; <em>k</em> = 0</span> <span class="math inline"><em>w</em><em>h</em><em>i</em><em>l</em><em>e</em></span> <span class="math inline">(<em>r</em><sub><em>k</em></sub> ≠ 0)</span> (or some threshold)</p>
<blockquote>
<p><span class="math inline">$\alpha_k = \frac{-r_k^Tp_k}{p_k^THp_k}$</span> $ x_{k+1} = x_k + _k P_k$ <span class="math inline"><em>r</em><sub><em>k</em> + 1</sub> = <em>H</em><em>x</em><sub><em>k</em> + 1</sub> + <em>c</em></span> <span class="math inline">$\beta_{k+1} = \frac {r_{k+1}^THp_{k}}{p_{k}^THp_{k}}$</span> <span class="math inline"><em>p</em><sub><em>k</em>+</sub> =  − <em>r</em><sub><em>k</em> + 1</sub> + <em>β</em><sub><em>k</em> + 1</sub><em>p</em><sub><em>k</em></sub></span> <span class="math inline"><em>k</em> = <em>k</em> + 1</span></p>
</blockquote>
</blockquote>
<blockquote>
<p><span class="math inline"><em>e</em><em>n</em><em>d</em></span></p>
</blockquote>
<p>This algorithm will ensure that:</p>
<blockquote>
<ul>
<li><span class="math inline">{<em>P</em><sub><em>i</em></sub>}</span> are H-conjugate</li>
</ul>
</blockquote>
<ul>
<li>The residuals are mututally orthogonal</li>
<li>Each residual <span class="math inline"><em>r</em><sub><em>k</em></sub></span> and the search direction <span class="math inline"><em>p</em><sub><em>k</em></sub></span> are contained in the <strong>Krylov subspace</strong> of degree <span class="math inline"><em>k</em></span> of <span class="math inline"><em>r</em><sub>0</sub></span>: <span class="math inline">𝒦(<em>r</em><sub>0</sub>; <em>k</em>) = <em>s</em><em>p</em><em>a</em><em>n</em>{<em>r</em><sub>0</sub>, <em>A</em><em>r</em><sub>0</sub>, ..., <em>A</em><sup><em>k</em></sup><em>r</em><sub>0</sub>}</span></li>
</ul>
<p>(How do we prove the current direction is orthogonal to all the previous directions? Proof using Krylov subspace - Manoj)</p>
<h3 id="writing-the-memory-efficient-cg-algorithm"><strong>Writing the memory efficient CG algorithm:</strong></h3>
<blockquote>
<p>We know <span class="math inline"><em>r</em><sub><em>k</em></sub><sup><em>T</em></sup><em>p</em><sub><em>i</em></sub> = 0, <em>i</em> = 0, ..., <em>k</em> − 1</span>. Also, <span class="math inline"><em>p</em><sub><em>k</em></sub> =  − <em>r</em><sub><em>k</em></sub> + <em>β</em><sub><em>k</em></sub><em>p</em><sub><em>k</em> − 1</sub></span>. Substituting in the equation for <span class="math inline"><em>α</em><sub><em>k</em></sub></span>, we get: <span class="math inline">$\alpha_k = \frac{r_k^Tr_k}{p_k^THp_k}$</span>. Doing the same in the equation for <span class="math inline"><em>β</em><sub><em>k</em> + 1</sub></span>, we get: <span class="math inline">$\beta_{k+1} = \frac{r_{k+1}^T r_{k+1}}{r_k^Tr_k}$</span>. The new modified algorithm is: <span class="math inline"><em>w</em><em>h</em><em>i</em><em>l</em><em>e</em></span> <span class="math inline">(<em>r</em><sub><em>k</em></sub> ≠ 0)</span> (or some threshold)</p>
<blockquote>
<p><span class="math inline">$\alpha_k = \frac{r_k^Tr_k}{p_k^THp_k}$</span> $ x_{k+1} = x_k + _k P_k$ <span class="math inline"><em>r</em><sub><em>k</em> + 1</sub> = <em>H</em><em>x</em><sub><em>k</em> + 1</sub> + <em>c</em></span> <span class="math inline">$\beta_{k+1} = \frac{r_{k+1}^T r_{k+1}}{r_k^Tr_k}$</span> <span class="math inline"><em>p</em><sub><em>k</em>+</sub> =  − <em>r</em><sub><em>k</em> + 1</sub> + <em>β</em><sub><em>k</em> + 1</sub><em>p</em><sub><em>k</em></sub></span> <span class="math inline"><em>k</em> = <em>k</em> + 1</span></p>
</blockquote>
</blockquote>
<blockquote>
<p><span class="math inline"><em>e</em><em>n</em><em>d</em></span></p>
</blockquote>
<p>We are trying to solve the quadratic minimization problem: <span class="math inline">$\begin{equation} \begin{array}{rrclcl} \displaystyle \min_{x} &amp; \frac{1}{2}x^THx + c^Tx\\ \end{array} \end{equation}$</span> which is equivalent to finding the solution of <span class="math inline"><em>H</em><em>x</em> =  − <em>c</em></span>. If we search along the conjugate directions of the <span class="math inline"><em>H</em></span> matrix, we can get the solution in n-iterations.</p>
<h2 id="complexity"><strong>Complexity:</strong></h2>
<p>Matrix-vector multiplication:</p>
<blockquote>
<ul>
<li>Dense: <span class="math inline"><em>n</em> × <em>n</em></span> and <span class="math inline"><em>n</em> × 1</span> <span class="math inline">𝒪(<em>n</em><sup>2</sup>)</span>.</li>
</ul>
</blockquote>
<ul>
<li>Sparse Matrix vector multiplication: <span class="math inline">𝒪(<em>m</em>)</span>, where <span class="math inline">′<em>m</em>′</span> is the non-zero elements in <span class="math inline"><em>H</em></span>.</li>
</ul>
<p>If we want the error (residual in this case) to decrease by a factor <span class="math inline"><em>ϵ</em></span>, say: <span class="math inline">∥<em>r</em><sub><em>k</em></sub>∥ ≤ <em>ϵ</em>∥<em>r</em><sub>0</sub>∥</span>, where <span class="math inline"><em>r</em><sub><em>k</em></sub></span> and <span class="math inline"><em>r</em><sub>0</sub></span> are the residuals in the <span class="math inline"><em>k</em><sup><em>t</em><em>h</em></sup></span> and the <span class="math inline">0<sup><em>t</em><em>h</em></sup></span> iterations.</p>
<blockquote>
<ul>
<li>S.D: Number of iterations = $ i $</li>
</ul>
</blockquote>
<ul>
<li><p>C.G: Number of iterations = $ i $ where</p>
<blockquote>
<blockquote>
<p><span class="math inline">𝒦</span> is the condition number of <span class="math inline"><em>H</em></span>. If we have a <span class="math inline"><em>d</em></span>-dimensional domain, to solve the boundary value problem. The condition number <span class="math inline">𝒦</span> of the matrix obtained by discretizing the second order ellyptic boundary value problem is of the order <span class="math inline">$\mathcal{O}(n^\frac{2}{d})$</span>, and the number of non-zero entries is of <span class="math inline">𝒪(<em>n</em>)</span>.</p>
</blockquote>
</blockquote></li>
</ul>
<blockquote>
<ul>
<li>S.D: Time complexity: <span class="math inline">𝒪(<em>m</em>𝒦)</span></li>
</ul>
</blockquote>
<ul>
<li><p>C.G: Time complexity: <span class="math inline">$\mathcal{O}(m\sqrt{\mathcal{K}})$</span></p></li>
<li><p>For a <span class="math inline">2 − <em>d</em></span> problem and <span class="math inline"><em>m</em></span> <span class="math inline"><em>ϵ</em></span> <span class="math inline">𝒪(<em>n</em>)</span>, we have the following:</p>
<blockquote>
<blockquote>
<ul>
<li>S.D: Time complexity: <span class="math inline">𝒪(<em>n</em><sup>2</sup>)</span></li>
<li>C.G: Time complexity: <span class="math inline">$\mathcal{O}(n^\frac{3}{2})$</span></li>
</ul>
</blockquote>
</blockquote></li>
</ul>
<blockquote>
<ul>
<li>Both have a space complexity of <span class="math inline">𝒪(<em>m</em>)</span>.</li>
</ul>
</blockquote>
<p>(Boundary value problem complexity).</p>
<h2 id="convergence-of-cg"><strong>Convergence of CG:</strong></h2>
<p>In exact arithmetic, the conjugate gradient method will terminate at the solution in at most n iterations. When the distribution of the eigenvalues of H has certain favorable features, the algorithm will identify the solution in many fewer than n iterations.</p>
<blockquote>
<ul>
<li>If <span class="math inline"><em>H</em></span> has only <span class="math inline"><em>r</em></span> distinct eigenvalues, then the CG iteration will terminate at the solution in at most <span class="math inline">′<em>r</em>′</span> iterations.</li>
</ul>
</blockquote>
<ul>
<li>It is generally true that if the eigenvalues occur in <span class="math inline"><em>r</em></span> distinct clusters, the CG iterates will approximately solve the problem in about <span class="math inline">′<em>r</em>′</span> steps</li>
</ul>
<h2 id="pre-conditioned-cg"><strong>Pre conditioned CG:</strong></h2>
<p>When the condition number <span class="math inline">𝒦</span> of <span class="math inline"><em>H</em></span> is reallt high, then we run into issues. We introduce a technique called as pre-conditioning. Here we transform the problem <span class="math inline"><em>H</em><em>x</em> =  − <em>c</em></span> with the inverse of <span class="math inline"><em>M</em></span> where <span class="math inline"><em>M</em></span> is SPD and invertible:</p>
<blockquote>
<ul>
<li><span class="math inline"><em>M</em><sup> − 1</sup><em>H</em><em>x</em> =  − <em>M</em><sup> − 1</sup><em>c</em></span></li>
</ul>
</blockquote>
<ul>
<li>We need: <span class="math inline">𝒦(<em>M</em><sup> − 1</sup><em>H</em>) ≪ 𝒦(<em>H</em>)</span> or the eigenvalues of <span class="math inline"><em>M</em><sup> − 1</sup><em>H</em></span> are clustered together, thereby ensuring faster convergence to the actual solution in <span class="math inline"><em>r</em></span> steps, if there are <span class="math inline"><em>r</em></span> clusters.</li>
</ul>
<p>Most of the standard solver packages use pre-conditioning for faster convergence. (Write the algo for preconditioned CG??)</p>
<h2 id="non-linear-cg"><strong>Non-Linear CG</strong></h2>
<p>If our objective function is not convex quadratic as we were dealing with till now, we will have to adapt our current way to fit it into a non-linear functions.</p>
<blockquote>
<ul>
<li>We were solving exactly for <span class="math inline"><em>α</em><sub><em>k</em></sub></span> till now, and we have to use the Backtracking line-search scheme to calculate <span class="math inline"><em>α</em></span>.</li>
<li>Instead of the residual <span class="math inline"><em>r</em><sub><em>k</em></sub></span>, we will use <span class="math inline">∇<em>f</em><sub><em>k</em></sub></span>, the gradient of the objective function at the current iterate.</li>
<li><span class="math inline"><em>β</em><sub><em>k</em> + 1</sub></span> has to be recalculated in terms of <span class="math inline">∇<em>f</em><sub><em>k</em></sub></span> and the new search direction <span class="math inline"><em>p</em><sub><em>k</em></sub></span>.</li>
</ul>
</blockquote>
<h3 id="how-should-beta_k-be-calculated"><strong>How should <span class="math inline"><em>β</em><sub><em>k</em></sub></span> be calculated:</strong></h3>
<blockquote>
<ul>
<li><p><strong>Fletcher - Reeves Method</strong></p>
<blockquote>
<p><span class="math inline">$\beta_{FR}^k = \frac{\nabla f_k^T \nabla f_k}{\nabla f_{k-1}^T \nabla f_{k-1}}$</span></p>
</blockquote></li>
<li><p><strong>Polak Ribere Method:</strong></p>
<blockquote>
<p><span class="math inline">$\beta_{PR}^k = \frac{\nabla f_k^T (\nabla f_k - \nabla f_{k-1})}{\nabla f_{k-1}^T \nabla f_{k-1}}$</span></p>
</blockquote></li>
<li><p><strong>Hestenes-Steifel Method:</strong></p>
<blockquote>
<p><span class="math inline">$\beta_{HS}^k = \frac{\nabla f_k^T (\nabla f_k - \nabla f_{k-1})}{(\nabla f_k - \nabla f_{k-1})^T p_{k-1}}$</span></p>
</blockquote></li>
</ul>
</blockquote>
</div><!-- /.entry-content -->
</article>
</section>
<section class="body" id="extras">
<div class="blogroll">
<h2>links</h2>
<ul>
<li><a href="https://getpelican.com/">Pelican</a></li>
<li><a href="https://www.python.org/">Python.org</a></li>
<li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
<li><a href="#">You can modify those links in your config file</a></li>
</ul>
</div><!-- /.blogroll -->
<div class="social">
<h2>social</h2>
<ul>
<li><a href="#">You can add links in your config file</a></li>
<li><a href="#">Another social link</a></li>
</ul>
</div><!-- /.social -->
</section><!-- /#extras -->
<footer class="body" id="contentinfo">
<address class="vcard body" id="about">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->
<p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
</footer><!-- /#contentinfo -->
</body>
</html>