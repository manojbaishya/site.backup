<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Pelican" name="generator"/>
<title>2nd CG Article</title>
<link href="https://manojbaishya.github.io/theme/css/main.css" rel="stylesheet"/>
<meta content="We saw how optimizing along conjugate directions helps in ariving at the solution faster. {Pi}, PiTHPj = diagonal Given an H, there does exists..." name="description"/>
</head>
<body class="home" id="index">
<header class="body" id="banner">
<h1><a href="https://manojbaishya.github.io/">Duality Blog</a></h1>
<nav><ul>
<li><a href="https://manojbaishya.github.io/pages/about.html">About</a></li>
<li class="active"><a href="https://manojbaishya.github.io/category/articles.html">articles</a></li>
<li><a href="https://manojbaishya.github.io/category/linear-algebra.html">linear-algebra</a></li>
</ul></nav>
</header><!-- /#banner -->
<section class="body" id="content">
<article>
<header>
<h1 class="entry-title">
<a href="https://manojbaishya.github.io/2nd-cg-article.html" rel="bookmark" title="Permalink to 2nd CG Article">2nd CG Article</a></h1>
</header>
<div class="entry-content">
<footer class="post-info">
<abbr class="published" title="2021-01-22T13:20:00+05:30">
                Published: Fri 22 January 2021
        </abbr>
<address class="vcard author">
                By                         <a class="url fn" href="https://manojbaishya.github.io/author/manoj-baishya.html">Manoj Baishya</a>
</address>
<p>In <a href="https://manojbaishya.github.io/category/articles.html">articles</a>.</p>
</footer><!-- /.post-info --> <p>We saw how optimizing along conjugate directions helps in ariving at the solution faster.</p>
<p><span class="math inline">{<em>P</em><sub><em>i</em></sub>},â€†<em>P</em><sub><em>i</em></sub><sup><em>T</em></sup><em>H</em><em>P</em><sup><em>j</em></sup>â€„=â€„<em>d</em><em>i</em><em>a</em><em>g</em><em>o</em><em>n</em><em>a</em><em>l</em></span></p>
<p>Given an H, there does exists these H conjugate directions. For a set <span class="math inline"><em>P</em><sub><em>i</em></sub></span>, it forms a linearly independent set or basis in <span class="math inline">â„›<sup><em>n</em></sup></span>.</p>
<h2 id="at-the-kth-iteration-we-will-search-along-the-dorection-p_k"><strong>1. At the kth iteration, we will search along the dorection <span class="math inline"><em>p</em><sub><em>k</em></sub></span></strong>:</h2>
<blockquote>
<p>$ x_{k+1} = x_k + _k p_k$ <span class="math inline"><em>Î±</em><sub><em>k</em></sub>â€„=â€„?</span> <span class="math inline">$\phi(\alpha) = \frac{1}{2}\alpha_kP_k^THP_k\alpha_k + c^T\alpha_kP_k + constant$</span> <span class="math inline">$\frac{\partial \phi}{\partial \alpha_k} = 0$</span> $_k =  $</p>
</blockquote>
<blockquote>
<p>Letâ€™s define the residual term at the kth iteration now: <span class="math inline"><em>r</em><sub><em>k</em></sub>â€„=â€„<em>H</em><em>x</em><sub><em>k</em></sub>â€…+â€…<em>c</em></span> <span class="math inline">$\alpha_k = \frac{-r_k^TP_k}{P_k^THP_k}$</span></p>
</blockquote>
<h2 id="residal-r_k"><strong>2. Residal <span class="math inline"><em>r</em><sub><em>k</em></sub></span></strong>:</h2>
<blockquote>
<p>$ x_{k+1} = x_k + _k P_k$ <span class="math inline"><em>H</em><em>x</em><sub><em>k</em>â€…+â€…1</sub>â€„=â€„<em>H</em><em>x</em><sub><em>k</em></sub>â€…+â€…<em>Î±</em><sub><em>k</em></sub><em>H</em><em>P</em><sub><em>k</em></sub></span> <span class="math inline"><em>H</em><em>x</em><sub><em>k</em>â€…+â€…1</sub>â€…+â€…<em>c</em>â€„=â€„<em>H</em><em>x</em><sub><em>k</em></sub>â€…+â€…<em>c</em>â€…+â€…<em>Î±</em><sub><em>k</em></sub><em>H</em><em>P</em><sub><em>k</em></sub></span> <span class="math inline"><em>r</em><sub><em>k</em>â€…+â€…1</sub>â€„=â€„<em>r</em><sub><em>k</em></sub>â€…+â€…<em>Î±</em><sub><em>k</em></sub><em>H</em><em>P</em><sub><em>k</em></sub></span></p>
</blockquote>
<h2 id="expanding-sunbspace-minimization-theorem"><strong>3. Expanding sunbspace Minimization theorem:</strong></h2>
<blockquote>
<p>Let <span class="math inline"><em>x</em><sub>0</sub>â€„âˆˆâ€„â„›<sup><em>n</em></sup></span>. Given <span class="math inline"><em>P</em><sub><em>K</em></sub></span>, which are H conjugate up until the kth iteration, we will generate <span class="math inline"><em>x</em><sub><em>k</em></sub></span>, and the seaquence <span class="math inline"><em>x</em><sub><em>k</em></sub>â€„=â€„<em>x</em><sub><em>k</em>â€…âˆ’â€…1</sub>â€…+â€…<em>Î±</em><sub><em>k</em>â€…âˆ’â€…1</sub><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub></span> is generates using the above update equation.</p>
</blockquote>
<blockquote>
<p>Then: <span class="math inline"><em>r</em><sub><em>k</em></sub><sup><em>T</em></sup><em>p</em><sub><em>i</em></sub>â€„=â€„0,â€†<em>i</em>â€„=â€„0,â€†...<em>k</em>â€…âˆ’â€…1</span> and <span class="math inline"><em>x</em><sub><em>k</em></sub></span> is the minimizer of <span class="math inline">$\phi(x) = \frac{1}{2}x^THx + c^Tx$</span> over the set <span class="math inline">{<em>x</em>,â€†<em>x</em>â€„=â€„<em>x</em><sub>0</sub>â€…+â€…<em>s</em><em>p</em><em>a</em><em>n</em>{<em>p</em><sub>0</sub>,â€†<em>p</em><sub>1</sub>,â€†...<em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub>}}</span>. The new residual <span class="math inline"><em>r</em><sub><em>k</em></sub></span> is going to be orthogonal to every other search direction used in the previous iterstions (provided that the search directions are H-conjugate}.</p>
</blockquote>
<blockquote>
<p>Proof:</p>
</blockquote>
<ol type="1">
<li>We will show that <span class="math inline"><em>xÌƒ</em></span> minimizes <span class="math inline"><em>Ï•</em>(<em>x</em>)</span> over <span class="math inline">{<em>x</em>,â€†<em>x</em>â€„=â€„<em>x</em><sub>0</sub>â€…+â€…<em>s</em><em>p</em><em>a</em><em>n</em>{<em>p</em><sub>0</sub>,â€†<em>p</em><sub>1</sub>,â€†...<em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub>}}</span>, iff <span class="math inline"><em>r</em>(<em>xÌƒ</em>)<sup><em>T</em></sup><em>p</em><sub><em>i</em></sub>â€„=â€„0,â€†<em>i</em>â€„=â€„0,â€†....,â€†<em>k</em>â€…âˆ’â€…1.</span></li>
</ol>
<blockquote>
<blockquote>
<p><span class="math inline"><em>Ïƒ</em>â€„=â€„(<em>Ïƒ</em><sub>0</sub>,â€†<em>Ïƒ</em><sub>1</sub>,â€†...,â€†<em>Ïƒ</em><sub><em>k</em>â€…âˆ’â€…1</sub>)<sup><em>T</em></sup></span> <span class="math inline"><em>h</em>(<em>Ïƒ</em>)</span> is a strict convex quadratic. <span class="math inline">$\frac{\partial h}{\partial \sigma_i} = 0$</span> <span class="math inline">âˆ‡<em>Ï•</em>(<em>x</em><sub>0</sub>â€…+â€…<em>Ïƒ</em><sub>0</sub><sup>*</sup><em>p</em><sub>0</sub>â€…+â€….....â€…+â€…<em>Ïƒ</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup>*</sup><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub>)<sup><em>T</em></sup><em>p</em><sub><em>i</em></sub>â€„=â€„0,â€†<em>i</em>â€„=â€„0,â€†...,â€†<em>k</em>â€…âˆ’â€…1</span> <span class="math inline"><em>xÌƒ</em>â€„=â€„<em>x</em><sub>0</sub>â€…+â€…<em>Ïƒ</em><sub>0</sub><sup>*</sup><em>p</em><sub>0</sub>â€…+â€….....â€…+â€…<em>Ïƒ</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup>*</sup><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub></span> <span class="math inline">âˆ‡<em>Ï•</em>(<em>xÌƒ</em>)<sup><em>T</em></sup><em>p</em><sub><em>i</em></sub>â€„=â€„0</span> <span class="math inline"><em>r</em>(<em>xÌƒ</em>)<sup><em>T</em></sup><em>p</em><sub><em>i</em></sub>â€„=â€„0,â€†<em>i</em>â€„=â€„0,â€†...,â€†<em>k</em>â€…âˆ’â€…1</span> We will use induction to show that <span class="math inline"><em>x</em><sub><em>k</em></sub></span> satisfies <span class="math inline"><em>r</em><sub><em>k</em></sub><sup><em>T</em></sup><em>p</em><sub><em>i</em></sub>â€„=â€„0</span>. For k = 1, <span class="math inline"><em>x</em><sub>1</sub>â€„=â€„<em>x</em><sub>0</sub>â€…+â€…<em>Î±</em><sub>0</sub><em>p</em><sub>0</sub></span> Induction Hypothesis: <span class="math inline"><em>r</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup><em>T</em></sup><em>p</em><sub><em>i</em></sub>â€„=â€„0,â€†<em>i</em>â€„=â€„0,â€†...,â€†<em>k</em>â€…âˆ’â€…2</span> <span class="math inline"><em>r</em><sub><em>k</em></sub>â€„=â€„<em>r</em><sub><em>k</em>â€…âˆ’â€…1</sub>â€…+â€…<em>Î±</em><sub><em>k</em>â€…âˆ’â€…1</sub><em>H</em><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub></span> <span class="math inline"><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup><em>T</em></sup><em>r</em><sub><em>k</em></sub>â€„=â€„<em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup><em>T</em></sup><em>r</em><sub><em>k</em>â€…âˆ’â€…1</sub>â€…+â€…<em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup><em>T</em></sup><em>Î±</em><sub><em>k</em>â€…âˆ’â€…1</sub><em>H</em><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub></span> <span class="math inline"><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup><em>T</em></sup><em>r</em><sub><em>k</em></sub>â€„=â€„0</span> For other vectors <span class="math inline"><em>p</em><sub><em>i</em></sub>,â€†<em>i</em>â€„=â€„0,â€†...,â€†<em>k</em>â€…âˆ’â€…2,</span> <span class="math inline"><em>p</em><sub><em>i</em></sub><sup><em>T</em></sup><em>r</em><sub><em>k</em></sub>â€„=â€„<em>p</em><sub><em>i</em></sub><sup><em>T</em></sup><em>r</em><sub><em>k</em>â€…âˆ’â€…1</sub>â€…+â€…<em>p</em><sub><em>i</em></sub><sup><em>T</em></sup><em>Î±</em><sub><em>k</em>â€…âˆ’â€…1</sub><em>H</em><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub></span> <span class="math inline"><em>p</em><sub><em>i</em></sub><sup><em>T</em></sup><em>r</em><sub><em>k</em></sub>â€„=â€„0</span> <span class="math inline"><em>r</em><sub><em>k</em></sub><sup><em>T</em></sup><em>p</em><sub><em>i</em></sub>â€„=â€„0,â€†<em>i</em>â€„=â€„0,â€†...,â€†<em>k</em>â€…âˆ’â€…1</span></p>
</blockquote>
</blockquote>
<h2 id="how-to-get-the-conjugate-directions"><strong>4. How to get the conjugate directions:</strong></h2>
<blockquote>
<ol type="a">
<li>Eigenvectors of the Hessian matrix:</li>
</ol>
<blockquote>
<p>Computationally very expensive to do. For large problems, this might be a bottleneck. Time and space complexity is very very high.</p>
</blockquote>
</blockquote>
<blockquote>
<ol start="2" type="a">
<li>Gram-Schmidt Orthogonalization:</li>
</ol>
<blockquote>
<p><span class="math inline"><em>P</em><sup><em>T</em></sup><em>P</em>â€„=â€„<em>I</em></span> Here we need: <span class="math inline"><em>P</em><sup><em>T</em></sup><em>H</em><em>P</em>â€„=â€„<em>D</em><em>i</em><em>a</em><em>g</em><em>o</em><em>n</em><em>a</em><em>l</em></span></p>
</blockquote>
</blockquote>
<p>Still not preferred, as we need to store all the directions, and only after the end of the algorithm, we will get the respective directions.</p>
<p>So we will take the knowledge that we have from the line search methods to solve this problem.</p>
<h2 id="conjugate-gradient-method"><strong>5. Conjugate Gradient Method:</strong></h2>
<blockquote>
<p>Calculate <span class="math inline"><em>p</em><sub><em>k</em></sub></span> only from <span class="math inline"><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub></span> and <span class="math inline">âˆ‡<em>f</em><sub><em>k</em></sub>â€„=â€„<em>r</em><sub><em>k</em></sub></span> <span class="math inline"><em>p</em><sub><em>k</em></sub>â€„=â€„â€…âˆ’â€…<em>r</em><sub><em>k</em></sub>â€…+â€…<em>Î²</em><sub><em>k</em></sub><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub></span> We define this new update scheme, and we set <span class="math inline"><em>Î²</em><sub><em>k</em></sub></span> such that <span class="math inline"><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup><em>T</em></sup><em>H</em><em>p</em><sub><em>k</em></sub>â€„=â€„0</span> We are using the direction of <span class="math inline"><em>r</em><sub><em>k</em></sub></span> and the previous search direction, in finding the new direction, in such a way that the new direction <span class="math inline"><em>p</em><sub><em>k</em></sub></span> is H-conjugate with the previous directions. <span class="math inline"><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup><em>T</em></sup><em>H</em><em>p</em><sub><em>k</em></sub>â€„=â€„â€…âˆ’â€…<em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup><em>T</em></sup><em>H</em><em>r</em><sub><em>k</em></sub>â€…+â€…<em>Î²</em><sub><em>k</em></sub><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub><sup><em>T</em></sup><em>H</em><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub></span> <span class="math inline">$\beta_k = \frac {p_{k-1}^THr_k}{p_{k-1}^THp_{k-1}} = \frac {r_k^THp_{k-1}}{p_{k-1}^THp_{k-1}}$</span> We can see that if we have an iterative method to calculate <span class="math inline"><em>H</em><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub></span>, that is enough.</p>
</blockquote>
<h3 id="conjugate-gradient-algorithm"><strong>Conjugate Gradient Algorithm:</strong></h3>
<blockquote>
<p>The first direction to search for is the steepest descent direction. <span class="math inline"><em>p</em><sub>0</sub>â€„=â€„â€…âˆ’â€…âˆ‡<em>f</em><sub>0</sub>â€„=â€„â€…âˆ’â€…<em>r</em><sub>0</sub></span>, Steepest descent direction. Given <span class="math inline"><em>x</em><sub>0</sub></span>, Set <span class="math inline"><em>r</em><sub>0</sub>â€„=â€„<em>H</em><em>x</em><sub>0</sub>â€…+â€…<em>c</em>;â€†<em>p</em><sub>0</sub>â€„=â€„â€…âˆ’â€…<em>r</em><sub>0</sub>;â€†<em>k</em>â€„=â€„0</span> <span class="math inline"><em>w</em><em>h</em><em>i</em><em>l</em><em>e</em></span> <span class="math inline">(<em>r</em><sub><em>k</em></sub>â€„â‰ â€„0)</span> (or some threshold)</p>
<blockquote>
<p><span class="math inline">$\alpha_k = \frac{-r_k^Tp_k}{p_k^THp_k}$</span> $ x_{k+1} = x_k + _k P_k$ <span class="math inline"><em>r</em><sub><em>k</em>â€…+â€…1</sub>â€„=â€„<em>H</em><em>x</em><sub><em>k</em>â€…+â€…1</sub>â€…+â€…<em>c</em></span> <span class="math inline">$\beta_{k+1} = \frac {r_{k+1}^THp_{k}}{p_{k}^THp_{k}}$</span> <span class="math inline"><em>p</em><sub><em>k</em>+</sub>â€„=â€„â€…âˆ’â€…<em>r</em><sub><em>k</em>â€…+â€…1</sub>â€…+â€…<em>Î²</em><sub><em>k</em>â€…+â€…1</sub><em>p</em><sub><em>k</em></sub></span> <span class="math inline"><em>k</em>â€„=â€„<em>k</em>â€…+â€…1</span></p>
</blockquote>
</blockquote>
<blockquote>
<p><span class="math inline"><em>e</em><em>n</em><em>d</em></span></p>
</blockquote>
<p>This algorithm will ensure that:</p>
<blockquote>
<ul>
<li><span class="math inline">{<em>P</em><sub><em>i</em></sub>}</span> are H-conjugate</li>
</ul>
</blockquote>
<ul>
<li>The residuals are mututally orthogonal</li>
<li>Each residual <span class="math inline"><em>r</em><sub><em>k</em></sub></span> and the search direction <span class="math inline"><em>p</em><sub><em>k</em></sub></span> are contained in the <strong>Krylov subspace</strong> of degree <span class="math inline"><em>k</em></span> of <span class="math inline"><em>r</em><sub>0</sub></span>: <span class="math inline">ğ’¦(<em>r</em><sub>0</sub>;â€†<em>k</em>)â€„=â€„<em>s</em><em>p</em><em>a</em><em>n</em>{<em>r</em><sub>0</sub>,â€†<em>A</em><em>r</em><sub>0</sub>,â€†...,â€†<em>A</em><sup><em>k</em></sup><em>r</em><sub>0</sub>}</span></li>
</ul>
<p>(How do we prove the current direction is orthogonal to all the previous directions? Proof using Krylov subspace - Manoj)</p>
<h3 id="writing-the-memory-efficient-cg-algorithm"><strong>Writing the memory efficient CG algorithm:</strong></h3>
<blockquote>
<p>We know <span class="math inline"><em>r</em><sub><em>k</em></sub><sup><em>T</em></sup><em>p</em><sub><em>i</em></sub>â€„=â€„0,â€†<em>i</em>â€„=â€„0,â€†...,â€†<em>k</em>â€…âˆ’â€…1</span>. Also, <span class="math inline"><em>p</em><sub><em>k</em></sub>â€„=â€„â€…âˆ’â€…<em>r</em><sub><em>k</em></sub>â€…+â€…<em>Î²</em><sub><em>k</em></sub><em>p</em><sub><em>k</em>â€…âˆ’â€…1</sub></span>. Substituting in the equation for <span class="math inline"><em>Î±</em><sub><em>k</em></sub></span>, we get: <span class="math inline">$\alpha_k = \frac{r_k^Tr_k}{p_k^THp_k}$</span>. Doing the same in the equation for <span class="math inline"><em>Î²</em><sub><em>k</em>â€…+â€…1</sub></span>, we get: <span class="math inline">$\beta_{k+1} = \frac{r_{k+1}^T r_{k+1}}{r_k^Tr_k}$</span>. The new modified algorithm is: <span class="math inline"><em>w</em><em>h</em><em>i</em><em>l</em><em>e</em></span> <span class="math inline">(<em>r</em><sub><em>k</em></sub>â€„â‰ â€„0)</span> (or some threshold)</p>
<blockquote>
<p><span class="math inline">$\alpha_k = \frac{r_k^Tr_k}{p_k^THp_k}$</span> $ x_{k+1} = x_k + _k P_k$ <span class="math inline"><em>r</em><sub><em>k</em>â€…+â€…1</sub>â€„=â€„<em>H</em><em>x</em><sub><em>k</em>â€…+â€…1</sub>â€…+â€…<em>c</em></span> <span class="math inline">$\beta_{k+1} = \frac{r_{k+1}^T r_{k+1}}{r_k^Tr_k}$</span> <span class="math inline"><em>p</em><sub><em>k</em>+</sub>â€„=â€„â€…âˆ’â€…<em>r</em><sub><em>k</em>â€…+â€…1</sub>â€…+â€…<em>Î²</em><sub><em>k</em>â€…+â€…1</sub><em>p</em><sub><em>k</em></sub></span> <span class="math inline"><em>k</em>â€„=â€„<em>k</em>â€…+â€…1</span></p>
</blockquote>
</blockquote>
<blockquote>
<p><span class="math inline"><em>e</em><em>n</em><em>d</em></span></p>
</blockquote>
<p>We are trying to solve the quadratic minimization problem: <span class="math inline">$\begin{equation} \begin{array}{rrclcl} \displaystyle \min_{x} &amp; \frac{1}{2}x^THx + c^Tx\\ \end{array} \end{equation}$</span> which is equivalent to finding the solution of <span class="math inline"><em>H</em><em>x</em>â€„=â€„â€…âˆ’â€…<em>c</em></span>. If we search along the conjugate directions of the <span class="math inline"><em>H</em></span> matrix, we can get the solution in n-iterations.</p>
<h2 id="complexity"><strong>Complexity:</strong></h2>
<p>Matrix-vector multiplication:</p>
<blockquote>
<ul>
<li>Dense: <span class="math inline"><em>n</em>â€…Ã—â€…<em>n</em></span> and <span class="math inline"><em>n</em>â€…Ã—â€…1</span> <span class="math inline">ğ’ª(<em>n</em><sup>2</sup>)</span>.</li>
</ul>
</blockquote>
<ul>
<li>Sparse Matrix vector multiplication: <span class="math inline">ğ’ª(<em>m</em>)</span>, where <span class="math inline">â€²<em>m</em>â€²</span> is the non-zero elements in <span class="math inline"><em>H</em></span>.</li>
</ul>
<p>If we want the error (residual in this case) to decrease by a factor <span class="math inline"><em>Ïµ</em></span>, say: <span class="math inline">âˆ¥<em>r</em><sub><em>k</em></sub>âˆ¥â€„â‰¤â€„<em>Ïµ</em>âˆ¥<em>r</em><sub>0</sub>âˆ¥</span>, where <span class="math inline"><em>r</em><sub><em>k</em></sub></span> and <span class="math inline"><em>r</em><sub>0</sub></span> are the residuals in the <span class="math inline"><em>k</em><sup><em>t</em><em>h</em></sup></span> and the <span class="math inline">0<sup><em>t</em><em>h</em></sup></span> iterations.</p>
<blockquote>
<ul>
<li>S.D: Number of iterations = $ i $</li>
</ul>
</blockquote>
<ul>
<li><p>C.G: Number of iterations = $ i $ where</p>
<blockquote>
<blockquote>
<p><span class="math inline">ğ’¦</span> is the condition number of <span class="math inline"><em>H</em></span>. If we have a <span class="math inline"><em>d</em></span>-dimensional domain, to solve the boundary value problem. The condition number <span class="math inline">ğ’¦</span> of the matrix obtained by discretizing the second order ellyptic boundary value problem is of the order <span class="math inline">$\mathcal{O}(n^\frac{2}{d})$</span>, and the number of non-zero entries is of <span class="math inline">ğ’ª(<em>n</em>)</span>.</p>
</blockquote>
</blockquote></li>
</ul>
<blockquote>
<ul>
<li>S.D: Time complexity: <span class="math inline">ğ’ª(<em>m</em>ğ’¦)</span></li>
</ul>
</blockquote>
<ul>
<li><p>C.G: Time complexity: <span class="math inline">$\mathcal{O}(m\sqrt{\mathcal{K}})$</span></p></li>
<li><p>For a <span class="math inline">2â€…âˆ’â€…<em>d</em></span> problem and <span class="math inline"><em>m</em></span> <span class="math inline"><em>Ïµ</em></span> <span class="math inline">ğ’ª(<em>n</em>)</span>, we have the following:</p>
<blockquote>
<blockquote>
<ul>
<li>S.D: Time complexity: <span class="math inline">ğ’ª(<em>n</em><sup>2</sup>)</span></li>
<li>C.G: Time complexity: <span class="math inline">$\mathcal{O}(n^\frac{3}{2})$</span></li>
</ul>
</blockquote>
</blockquote></li>
</ul>
<blockquote>
<ul>
<li>Both have a space complexity of <span class="math inline">ğ’ª(<em>m</em>)</span>.</li>
</ul>
</blockquote>
<p>(Boundary value problem complexity).</p>
<h2 id="convergence-of-cg"><strong>Convergence of CG:</strong></h2>
<p>In exact arithmetic, the conjugate gradient method will terminate at the solution in at most n iterations. When the distribution of the eigenvalues of H has certain favorable features, the algorithm will identify the solution in many fewer than n iterations.</p>
<blockquote>
<ul>
<li>If <span class="math inline"><em>H</em></span> has only <span class="math inline"><em>r</em></span> distinct eigenvalues, then the CG iteration will terminate at the solution in at most <span class="math inline">â€²<em>r</em>â€²</span> iterations.</li>
</ul>
</blockquote>
<ul>
<li>It is generally true that if the eigenvalues occur in <span class="math inline"><em>r</em></span> distinct clusters, the CG iterates will approximately solve the problem in about <span class="math inline">â€²<em>r</em>â€²</span> steps</li>
</ul>
<h2 id="pre-conditioned-cg"><strong>Pre conditioned CG:</strong></h2>
<p>When the condition number <span class="math inline">ğ’¦</span> of <span class="math inline"><em>H</em></span> is reallt high, then we run into issues. We introduce a technique called as pre-conditioning. Here we transform the problem <span class="math inline"><em>H</em><em>x</em>â€„=â€„â€…âˆ’â€…<em>c</em></span> with the inverse of <span class="math inline"><em>M</em></span> where <span class="math inline"><em>M</em></span> is SPD and invertible:</p>
<blockquote>
<ul>
<li><span class="math inline"><em>M</em><sup>â€…âˆ’â€…1</sup><em>H</em><em>x</em>â€„=â€„â€…âˆ’â€…<em>M</em><sup>â€…âˆ’â€…1</sup><em>c</em></span></li>
</ul>
</blockquote>
<ul>
<li>We need: <span class="math inline">ğ’¦(<em>M</em><sup>â€…âˆ’â€…1</sup><em>H</em>)â€„â‰ªâ€„ğ’¦(<em>H</em>)</span> or the eigenvalues of <span class="math inline"><em>M</em><sup>â€…âˆ’â€…1</sup><em>H</em></span> are clustered together, thereby ensuring faster convergence to the actual solution in <span class="math inline"><em>r</em></span> steps, if there are <span class="math inline"><em>r</em></span> clusters.</li>
</ul>
<p>Most of the standard solver packages use pre-conditioning for faster convergence. (Write the algo for preconditioned CG??)</p>
<h2 id="non-linear-cg"><strong>Non-Linear CG</strong></h2>
<p>If our objective function is not convex quadratic as we were dealing with till now, we will have to adapt our current way to fit it into a non-linear functions.</p>
<blockquote>
<ul>
<li>We were solving exactly for <span class="math inline"><em>Î±</em><sub><em>k</em></sub></span> till now, and we have to use the Backtracking line-search scheme to calculate <span class="math inline"><em>Î±</em></span>.</li>
<li>Instead of the residual <span class="math inline"><em>r</em><sub><em>k</em></sub></span>, we will use <span class="math inline">âˆ‡<em>f</em><sub><em>k</em></sub></span>, the gradient of the objective function at the current iterate.</li>
<li><span class="math inline"><em>Î²</em><sub><em>k</em>â€…+â€…1</sub></span> has to be recalculated in terms of <span class="math inline">âˆ‡<em>f</em><sub><em>k</em></sub></span> and the new search direction <span class="math inline"><em>p</em><sub><em>k</em></sub></span>.</li>
</ul>
</blockquote>
<h3 id="how-should-beta_k-be-calculated"><strong>How should <span class="math inline"><em>Î²</em><sub><em>k</em></sub></span> be calculated:</strong></h3>
<blockquote>
<ul>
<li><p><strong>Fletcher - Reeves Method</strong></p>
<blockquote>
<p><span class="math inline">$\beta_{FR}^k = \frac{\nabla f_k^T \nabla f_k}{\nabla f_{k-1}^T \nabla f_{k-1}}$</span></p>
</blockquote></li>
<li><p><strong>Polak Ribere Method:</strong></p>
<blockquote>
<p><span class="math inline">$\beta_{PR}^k = \frac{\nabla f_k^T (\nabla f_k - \nabla f_{k-1})}{\nabla f_{k-1}^T \nabla f_{k-1}}$</span></p>
</blockquote></li>
<li><p><strong>Hestenes-Steifel Method:</strong></p>
<blockquote>
<p><span class="math inline">$\beta_{HS}^k = \frac{\nabla f_k^T (\nabla f_k - \nabla f_{k-1})}{(\nabla f_k - \nabla f_{k-1})^T p_{k-1}}$</span></p>
</blockquote></li>
</ul>
</blockquote>
</div><!-- /.entry-content -->
</article>
</section>
<section class="body" id="extras">
<div class="blogroll">
<h2>links</h2>
<ul>
<li><a href="https://getpelican.com/">Pelican</a></li>
<li><a href="https://www.python.org/">Python.org</a></li>
<li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
<li><a href="#">You can modify those links in your config file</a></li>
</ul>
</div><!-- /.blogroll -->
<div class="social">
<h2>social</h2>
<ul>
<li><a href="#">You can add links in your config file</a></li>
<li><a href="#">Another social link</a></li>
</ul>
</div><!-- /.social -->
</section><!-- /#extras -->
<footer class="body" id="contentinfo">
<address class="vcard body" id="about">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->
<p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
</footer><!-- /#contentinfo -->
</body>
</html>