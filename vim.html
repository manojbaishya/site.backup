<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="Pelican" name="generator"/>
<title>Vim</title>
<link href="https://manojbaishya.github.io/theme/css/main.css" rel="stylesheet"/>
<meta content="We saw how optimizing along conjugate directions helps in ariving at the solution faster. ${P_i}, P_i^THP^j = diagonal$ Given an H, there does..." name="description"/>
</head>
<body class="home" id="index">
<header class="body" id="banner">
<h1><a href="https://manojbaishya.github.io/">Duality Blog</a></h1>
<nav><ul>
<li><a href="https://manojbaishya.github.io/pages/about.html">About</a></li>
<li class="active"><a href="https://manojbaishya.github.io/category/articles.html">articles</a></li>
<li><a href="https://manojbaishya.github.io/category/linear-algebra.html">linear-algebra</a></li>
</ul></nav>
</header><!-- /#banner -->
<section class="body" id="content">
<article>
<header>
<h1 class="entry-title">
<a href="https://manojbaishya.github.io/vim.html" rel="bookmark" title="Permalink to Vim">Vim</a></h1>
</header>
<div class="entry-content">
<footer class="post-info">
<abbr class="published" title="2021-01-22T13:20:00+05:30">
                Published: Fri 22 January 2021
        </abbr>
<address class="vcard author">
                By                         <a class="url fn" href="https://manojbaishya.github.io/author/manoj-baishya.html">Manoj Baishya</a>
</address>
<p>In <a href="https://manojbaishya.github.io/category/articles.html">articles</a>.</p>
</footer><!-- /.post-info --> <p>We saw how optimizing along conjugate directions helps in ariving at the
solution faster.</p>
<p>${P_i}, P_i^THP^j = diagonal$</p>
<p>Given an H, there does exists these H conjugate directions. For a set
${P_i}$, it forms a linearly independent set or basis in
$mathcal{R}^n$.</p>
<div class="section" id="id1">
<span id="at-the-kth-iteration-we-will-search-along-the-dorection-p-k"></span><h2><strong>1. At the kth iteration, we will search along the dorection $p_k$</strong>:</h2>
<blockquote>
$ x_{k+1} = x_k + \alpha_k p_k$ $alpha_k = ?$ $phi(alpha) =
\frac{1}{2}alpha_kP_k^THP_kalpha_k + c^Talpha_kP_k + constant$
$frac{partial \phi}{partial \alpha_k} = 0$ $alpha_k = \frac
{-(c+Hx_k)^TP_k}{P_K^THP_k} $</blockquote>
<!-- -->
<blockquote>
Let's define the residual term at the kth iteration now: $r_k = Hx_k
+ c$ $alpha_k = \frac{-r_k^TP_k}{P_k^THP_k}$</blockquote>
</div>
<div class="section" id="id2">
<span id="residal-r-k"></span><h2><strong>2. Residal $r_k$</strong>:</h2>
<blockquote>
$ x_{k+1} = x_k + \alpha_k P_k$ $Hx_{k+1} = Hx_k + \alpha_kHP_k$
$Hx_{k+1} + c = Hx_k + c + \alpha_kHP_k$ $r_{k+1} = r_k +
\alpha_kHP_k$</blockquote>
</div>
<div class="section" id="id3">
<span id="expanding-sunbspace-minimization-theorem"></span><h2><strong>3. Expanding sunbspace Minimization theorem:</strong></h2>
<blockquote>
Let $x_0 \in \mathcal{R}^n$. Given ${P_K}$, which are H conjugate
up until the kth iteration, we will generate $x_k$, and the seaquence
$x_{k} = x_{k-1} + \alpha_{k-1} p_{k-1}$ is generates using the
above update equation.</blockquote>
<!-- -->
<blockquote>
<p>Then: $r_k^Tp_i = 0, i = 0,...k-1$ and $x_k$ is the minimizer of
$phi(x) = \frac{1}{2}x^THx + c^Tx$ over the set ${x, x = x_0 +
span{p_0, p_1,...p_{k-1}}}$. The new residual $r_k$ is going to be
orthogonal to every other search direction used in the previous
iterstions (provided that the search directions are H-conjugate}.</p>
<p>Proof:</p>
</blockquote>
<ol class="arabic simple">
<li>We will show that $tilde{x}$ minimizes $phi(x)$ over ${x, x = x_0 +
span{p_0, p_1,...p_{k-1}}}$, iff $r(tilde{x})^Tp_i = 0, i = 0,....,
k-1.$</li>
</ol>
<!-- -->
<blockquote>
$sigma = (sigma_0, \sigma_1, ..., \sigma_{k-1})^T$ $h(sigma)$
is a strict convex quadratic. $frac{partial h}{partial
\sigma_i} = 0$ $nabla \phi(x_0 + \sigma_0^*p_0 + .....+
\sigma_{k-1}^*p_{k-1})^Tp_i = 0, i = 0,...,k-1$ $tilde{x} = x_0
+ \sigma_0^*p_0 + .....+ \sigma_{k-1}^*p_{k-1}$ $nabla
\phi(tilde{x})^Tp_i = 0$ $r(tilde{x})^Tp_i = 0, i = 0,...,k-1$
We will use induction to show that $x_k$ satisfies $r_k^Tp_i = 0$.
For k = 1, $x_1 = x_0 + \alpha_0p_0$ Induction Hypothesis:
$r_{k-1}^Tp_i = 0, i = 0, ... , k-2$ $r_k = r_{k-1} +
\alpha_{k-1}Hp_{k-1}$ $p_{k-1}^Tr_k = p_{k-1}^Tr_{k-1} +
p_{k-1}^Talpha_{k-1}Hp_{k-1}$ $p_{k-1}^Tr_k = 0$ For other
vectors $p_i, i = 0, ..., k-2,$ $p_i^Tr_k = p_i^Tr_{k-1} +
p_i^Talpha_{k-1}Hp_{k-1}$ $p_i^Tr_k = 0$ $r_k^Tp_i = 0, i =
0,...,k-1$</blockquote>
</div>
<div class="section" id="id4">
<span id="how-to-get-the-conjugate-directions"></span><h2><strong>4. How to get the conjugate directions:</strong></h2>
<blockquote>
<ol class="loweralpha">
<li><p class="first">Eigenvectors of the Hessian matrix:</p>
<p>Computationally very expensive to do. For large problems, this
might be a bottleneck. Time and space complexity is very very
high.</p>
</li>
</ol>
</blockquote>
<!-- -->
<blockquote>
<ol class="loweralpha" start="2">
<li><p class="first">Gram-Schmidt Orthogonalization:</p>
<p>$P^TP = I$ Here we need: $P^THP = Diagonal$</p>
</li>
</ol>
</blockquote>
<p>Still not preferred, as we need to store all the directions, and only
after the end of the algorithm, we will get the respective directions.</p>
<p>So we will take the knowledge that we have from the line search methods
to solve this problem.</p>
</div>
<div class="section" id="id5">
<span id="conjugate-gradient-method"></span><h2><strong>5. Conjugate Gradient Method:</strong></h2>
<blockquote>
Calculate $p_k$ only from $p_{k-1}$ and $nabla f_k = r_k$ $p_k =
-r_k + \beta_k p_{k-1}$ We define this new update scheme, and we set
$beta_k$ such that $p_{k-1}^THp_k = 0$ We are using the direction of
$r_k$ and the previous search direction, in finding the new
direction, in such a way that the new direction $p_k$ is H-conjugate
with the previous directions. $p_{k-1}^THp_k = -p_{k-1}^THr_k +
\beta_k p_{k-1}^THp_{k-1}$ $beta_k = \frac
{p_{k-1}^THr_k}{p_{k-1}^THp_{k-1}} = \frac
{r_k^THp_{k-1}}{p_{k-1}^THp_{k-1}}$ We can see that if we have an
iterative method to calculate $Hp_{k-1}$, that is enough.</blockquote>
<div class="section" id="conjugate-gradient-algorithm">
<h3><strong>Conjugate Gradient Algorithm:</strong></h3>
<blockquote>
<p>The first direction to search for is the steepest descent direction.
$p_0 = -nabla f_0 = -r_0$, Steepest descent direction. Given $x_0$,
Set $r_0 = H x_0 + c; p_0 = -r_0; k=0$ $while$ $(r_k \neq 0 )$ (or
some threshold)</p>
<blockquote>
$alpha_k = \frac{-r_k^Tp_k}{p_k^THp_k}$ $ x_{k+1} = x_k +
\alpha_k P_k$ $r_{k+1} = Hx_{k+1} + c$ $beta_{k+1} = \frac
{r_{k+1}^THp_{k}}{p_{k}^THp_{k}}$ $p_{k+} = -r_{k+1} +
\beta_{k+1} p_{k}$ $k = k+1$</blockquote>
</blockquote>
<!-- -->
<blockquote>
$end$</blockquote>
<p>This algorithm will ensure that:</p>
<blockquote>
<ul class="simple">
<li>${ P_i }$ are H-conjugate</li>
</ul>
</blockquote>
<ul class="simple">
<li>The residuals are mututally orthogonal</li>
<li>Each residual $r_k$ and the search direction $p_k$ are contained in
the <strong>Krylov subspace</strong> of degree $k$ of $r_0$: $mathcal{K}(r_0;k) =
span{r_0, Ar_0, ..., A^kr_0}$</li>
</ul>
<p>(How do we prove the current direction is orthogonal to all the previous
directions? Proof using Krylov subspace - Manoj)</p>
</div>
<div class="section" id="writing-the-memory-efficient-cg-algorithm">
<h3><strong>Writing the memory efficient CG algorithm:</strong></h3>
<blockquote>
<p>We know $r_k^Tp_i = 0, i = 0,...,k-1$. Also, $p_k = -r_k + \beta_k
p_{k-1}$. Substituting in the equation for $alpha_k$, we get:
$alpha_k = \frac{r_k^Tr_k}{p_k^THp_k}$. Doing the same in the
equation for $beta_{k+1}$, we get: $beta_{k+1} = \frac{r_{k+1}^T
r_{k+1}}{r_k^Tr_k}$. The new modified algorithm is: $while$ $(r_k
\neq 0 )$ (or some threshold)</p>
<blockquote>
$alpha_k = \frac{r_k^Tr_k}{p_k^THp_k}$ $ x_{k+1} = x_k +
\alpha_k P_k$ $r_{k+1} = Hx_{k+1} + c$ $beta_{k+1} =
\frac{r_{k+1}^T r_{k+1}}{r_k^Tr_k}$ $p_{k+} = -r_{k+1} +
\beta_{k+1} p_{k}$ $k = k+1$</blockquote>
</blockquote>
<!-- -->
<blockquote>
$end$</blockquote>
<p>We are trying to solve the quadratic minimization problem:
$begin{equation} \begin{array}{rrclcl} \displaystyle \min_{x} &amp;
\frac{1}{2}x^THx + c^Tx\ \end{array} \end{equation}$ which is
equivalent to finding the solution of $Hx = -c$. If we search along the
conjugate directions of the $H$ matrix, we can get the solution in
n-iterations.</p>
</div>
</div>
<div class="section" id="complexity">
<h2><strong>Complexity:</strong></h2>
<p>Matrix-vector multiplication:</p>
<blockquote>
<ul class="simple">
<li>Dense: $n \times n$ and $n \times 1$ $mathcal{O}(n^2)$.</li>
</ul>
</blockquote>
<ul class="simple">
<li>Sparse Matrix vector multiplication: $mathcal{O}(m)$, where $'m'$ is
the non-zero elements in $H$.</li>
</ul>
<p>If we want the error (residual in this case) to decrease by a factor
$epsilon$, say: $| r_k| \leq \epsilon | r_0 |$, where $r_k$ and
$r_0$ are the residuals in the $k^{th}$ and the $0^{th}$ iterations.</p>
<blockquote>
<ul class="simple">
<li>S.D: Number of iterations = $ i \leq
\frac{1}{2}mathcal{K}ln{frac{1}{epsilon}}$</li>
</ul>
</blockquote>
<ul>
<li><p class="first">C.G: Number of iterations = $ i \leq
\frac{1}{2}sqrt{mathcal{K}}ln{frac{2}{epsilon}}$ where</p>
<blockquote>
<p>$mathcal{K}$ is the condition number of $H$. If we have a
$d$-dimensional domain, to solve the boundary value problem.
The condition number $mathcal{K}$ of the matrix obtained by
discretizing the second order ellyptic boundary value problem
is of the order $mathcal{O}(n^frac{2}{d})$, and the number of
non-zero entries is of $mathcal{O}(n)$.</p>
</blockquote>
</li>
</ul>
<!-- -->
<blockquote>
<ul class="simple">
<li>S.D: Time complexity: $mathcal{O}(mmathcal{K})$</li>
</ul>
</blockquote>
<ul>
<li><p class="first">C.G: Time complexity: $mathcal{O}(msqrt{mathcal{K}})$</p>
</li>
<li><p class="first">For a $2-d$ problem and $m$ $epsilon$ $mathcal{O}(n)$, we have the
following:</p>
<blockquote>
<ul class="simple">
<li>S.D: Time complexity: $mathcal{O}(n^2)$</li>
<li>C.G: Time complexity: $mathcal{O}(n^frac{3}{2})$</li>
</ul>
</blockquote>
</li>
</ul>
<!-- -->
<blockquote>
<ul class="simple">
<li>Both have a space complexity of $mathcal{O}(m)$.</li>
</ul>
</blockquote>
<p>(Boundary value problem complexity).</p>
</div>
<div class="section" id="convergence-of-cg">
<h2><strong>Convergence of CG:</strong></h2>
<p>In exact arithmetic, the conjugate gradient method will terminate at the
solution in at most n iterations. When the distribution of the
eigenvalues of H has certain favorable features, the algorithm will
identify the solution in many fewer than n iterations.</p>
<blockquote>
<ul class="simple">
<li>If $H$ has only $r$ distinct eigenvalues, then the CG iteration
will terminate at the solution in at most $'r'$ iterations.</li>
</ul>
</blockquote>
<ul class="simple">
<li>It is generally true that if the eigenvalues occur in $r$ distinct
clusters, the CG iterates will approximately solve the problem in
about $'r'$ steps</li>
</ul>
</div>
<div class="section" id="pre-conditioned-cg">
<h2><strong>Pre conditioned CG:</strong></h2>
<p>When the condition number $mathcal{K}$ of $H$ is reallt high, then we
run into issues. We introduce a technique called as pre-conditioning.
Here we transform the problem $Hx = -c$ with the inverse of $M$ where
$M$ is SPD and invertible:</p>
<blockquote>
<ul class="simple">
<li>$M^{-1}Hx = -M^{-1}c$</li>
</ul>
</blockquote>
<ul class="simple">
<li>We need: $mathcal{K}(M^{-1}H) \ll \mathcal{K}(H)$ or the
eigenvalues of $M^{-1}H$ are clustered together, thereby ensuring
faster convergence to the actual solution in $r$ steps, if there are
$r$ clusters.</li>
</ul>
<p>Most of the standard solver packages use pre-conditioning for faster
convergence. (Write the algo for preconditioned CG??)</p>
</div>
<div class="section" id="non-linear-cg">
<h2><strong>Non-Linear CG</strong></h2>
<p>If our objective function is not convex quadratic as we were dealing
with till now, we will have to adapt our current way to fit it into a
non-linear functions.</p>
<blockquote>
<ul class="simple">
<li>We were solving exactly for $alpha_k$ till now, and we have to
use the Backtracking line-search scheme to calculate $alpha$.</li>
<li>Instead of the residual $r_k$, we will use $nabla f_k$, the
gradient of the objective function at the current iterate.</li>
<li>$beta_{k+1}$ has to be recalculated in terms of $nabla f_k$ and
the new search direction $p_k$.</li>
</ul>
</blockquote>
<div class="section" id="how-should-beta-k-be-calculated">
<h3><strong>How should $beta_k$ be calculated:</strong></h3>
<blockquote>
<ul>
<li><p class="first"><strong>Fletcher - Reeves Method</strong></p>
<blockquote>
<p>$beta_{FR}^k = \frac{nabla f_k^T \nabla f_k}{nabla
f_{k-1}^T \nabla f_{k-1}}$</p>
</blockquote>
</li>
<li><p class="first"><strong>Polak Ribere Method:</strong></p>
<blockquote>
<p>$beta_{PR}^k = \frac{nabla f_k^T (nabla f_k - \nabla
f_{k-1})}{nabla f_{k-1}^T \nabla f_{k-1}}$</p>
</blockquote>
</li>
<li><p class="first"><strong>Hestenes-Steifel Method:</strong></p>
<blockquote>
<p>$beta_{HS}^k = \frac{nabla f_k^T (nabla f_k - \nabla
f_{k-1})}{(nabla f_k - \nabla f_{k-1})^T p_{k-1}}$</p>
</blockquote>
</li>
</ul>
</blockquote>
</div>
</div>
</div><!-- /.entry-content -->
</article>
</section>
<section class="body" id="extras">
<div class="blogroll">
<h2>links</h2>
<ul>
<li><a href="https://getpelican.com/">Pelican</a></li>
<li><a href="https://www.python.org/">Python.org</a></li>
<li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
<li><a href="#">You can modify those links in your config file</a></li>
</ul>
</div><!-- /.blogroll -->
<div class="social">
<h2>social</h2>
<ul>
<li><a href="#">You can add links in your config file</a></li>
<li><a href="#">Another social link</a></li>
</ul>
</div><!-- /.social -->
</section><!-- /#extras -->
<footer class="body" id="contentinfo">
<address class="vcard body" id="about">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->
<p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
</footer><!-- /#contentinfo -->
</body>
</html>